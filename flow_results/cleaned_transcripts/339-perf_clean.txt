There has been a bunch of renewed interest in making Python faster.
While for some of us, Python is already plenty fast for others, such as those in data science, scientific computing, and even large tech companies, making Python even a little faster would be a big deal.
This episode is the first of several that dive into some of the active efforts to increase the speed of Python while maintaining compatibility with existing code and packages.
And who better to help kick this off than Gitovan Rossum and Mark Shannon?
They both joined us to share their project to make Python faster. I'm sure you'll love hearing about what they're up to.
This is Talk Python to me. Episode 339, recorded November 1, 2021. Welcome to Talk Python, a weekly podcast on Python.
This is your host, Michael Kennedy.
Follow me on Twitter, where I'm @mkennedy and keep up with the show and listen to past episodes at Talkpython FM and follow the show on Twitter via @ Talkpython.
We started streaming most of our episodes live on YouTube, subscribe to our YouTube channel over at 'talk python.fm/youtube' to get notified about upcoming shows and be part of that episode.
This episode is brought to you by Shortcut and Linode and the transcripts are sponsored by 'Assembly AI' Mark, Guidow.
Fantastic to have you here.
I'm so excited about all the things that are happening around Python performance.
I feel like there's just a bunch of new ideas springing up and people working on it, and it's exciting times, definitely.
You two are, of course, right at the center of it. But before we talk about the performance work that you are doing, as well as some of the other initiatives going along, maybe in parallel there.
Let's just get started with a little bit of background on you.
Guido, you've been on the show before.
Creator of Python. You hardly need an introduction to most people out there, but you have recently made a couple of big changes in your life. I thought I'd just ask you how that's going.
You retired and we're all super happy for you on that. And then you said, you know what kind of want to play with code some more? And now you're at Microsoft. What's the story there?
Oh, I just like the idea of retiring. So I try to see how many times in a lifetime I can retire.
Starting with my retirement from BDFL didn't stop me from staying super active in the community.
But when I retired from Dropbox a little over two years ago, I really thought that that was it that I believed it and everybody else believed it to Dropbox.
Certainly believed it.
They were very sad to see me go.
I was sad to go, but I thought it was time and I had a few great months decompressing going on bike rides with my wife and family.
Fun stuff.
And then the pandemic hit and a bunch of things got harder.
Fortunately, the bike ride eventually got restored. But other activities, like eating out was a lot more stressful.
Basically, just life was a lot more stressful in general.
Right.
And human interaction was definitely drunken down to a kernel.
Yeah.
And somehow I thought, Well, I want to have something to do.
I want to do more sort of software development in the team.
And the Python core development team didn't really cut it for me because it's sort of diffuse and volunteer based.
And sometimes you get stuck waiting for months for the steering Council to sort of approve of or reject a certain idea that you worked on.
So I asked around and I found that Microsoft was super interested in hiring me, and that was now exactly a month.
Tomorrow.
A year ago, I started at Microsoft.
In the beginning, I just had to find my way around at Microsoft.
Eventually I figured I should pick a project.
And after looking around and realizing I couldn't really sort of turn the world of machine learning upside down, I figured I'd stay closer to home and see if Microsoft was interested in funding a team working on speeding up C Python. And I was actually inspired by Mark's proposals that were going around at the time.
So I convinced Microsoft to sort of start a small team and get Mark on board.
Yeah, that's fantastic.
I also feel a little bit like machine learning is amazing, but I don't have a lot of experience with it. And whenever I work with it, I always kind of feel on the outside of it. But this core performance of Python that helps everybody, including even Microsoft, it maybe saves them absolutely.
Energy and Azure when they're running Python workloads or whatever.
So you enjoying your time. Are you happy there?
I'm very happy.
Yeah.
A lot of freedom to basically pursue what you are. Right.
And it's nice that the new Microsoft is very open source friendly, at least in many cases.
Obviously not everywhere.
But our Department is very opensource friendly things like Visual Studio code are all open source.
And so there was great support with management for sort of the way I said I wanted to do this project, which is completely out in the open.
Everything we do is sort of just merged into Main as soon as we can.
We work with the core developers.
We don't have, like, a private fork of Python where we do amazing stuff.
And then we knock on the steering console door and say, hey, we'd like to merge this.
Yeah.
You're not going to drop six months of work just in one block, right.
It's there for everyone to see.
Exactly.
I think that's really positive.
And wow, what a change.
Not just for Microsoft, but so many companies to work that way compared to ten or 15 years ago.
Yeah.
Absolutely.
Now, before I get to Mark, I just want to bunch of people are excited that you're here.
And Luis, out in the audience said, wow, it's Guido. I can't thank you enough for your amazing Python and all the community.
Great to hear, Mark, how about you? How did you get into this Python performance thing?
I know you did some stuff with HotPy back in the day.
Yeah, that was sort of my PhD work, so I guess I kind of go into the performance almost before the Python.
So things of compiler work Masters, and obviously just you need to write scripts and just get stuff done.
Just Python is just a language to get stuff done. And then it's that I think I'm in Regal, I think one of his sort of credit in one of his papers or something.
Thank you for Python for being such a great language to use and such a challenge to optimize.
So it's definitely good if you're coming at it from a provides this great intellectual challenge when you're actually trying to optimize it.
And it's a really nice language to use as well. So it's doubly good.
It is definitely good before we move on. Really quick. Paul, ever it says it's really impressive how the in the open work has been done.
I totally agree. Hi, Paul.
Yeah. Keep that going. Hey, Paul, happy to see you here.
We're going to talk about making Python faster, but I want to start this conversation.
It's a bit of a hypothetical question, but sort of set the stage and ask, how much does Python really need to be faster?
Because on one hand, sure, there's a lot more performance we can do if you're going to say, well, we're going to solve the in body problem using C++ or C# versus Python. It's going to be faster with the native value types and whatnot on the other, people are building amazing software that runs really fast with Python already.
We've got the C optimizations for things like NumPy and SQLAlchemies transformation layer, serialization layer, and so on.
So a lot of times that kind of brings it back to see performance. So how much do you think Python really needs to be optimized already?
Now that more is always better. Faster is always better. But I just kind of want to set the stage and get your two thoughts on that.
I always think back to my experience at Dropbox, where there was a large server called the Meta Server, which did sort of all the Serve side work like anything that hits www.
dropbox.com hits that server, and that server was initially a small prototype written in Python.
The client was actually also a small prototype written in Python.
And to this day, both the server and the client at Dropbox as far as I know. And unless in the last two years, they totally ripped it apart, but I don't think they did.
They tweaked it, but it's still all now very large Python applications.
And so Dropbox really sort of feels the speed of Python in its budget because they have thousands.
I don't know how many thousands of machines that all run this enormous Python application.
Right. If it was four times faster, that's not just a quarter of the machines that's less DevOps, less admin, all sorts of stuff. Right.
Even if it was 4% faster, they would notice.
Yeah.
The other area where I think it's really relevant has to do with the multicore side of things.
I have a PC over there.
16 cores.
My new laptop has ten cores, although with Python, it's hard to take true advantage of that side of modern CPU performance if it's not IO bound, right?
Yeah.
I don't know how deep you want me to go into that and Mark and stop me if I'm going too deep, too. But there are existing patterns that work reasonably well.
If you have a server application that handles multiple, fairly independent requests, like if you're building a multicore web application, you can use multi processing or pre forking or variety of ways of running a Python interpreter on each core that you have each independently handling requests.
And you can do that. If you have 64 cores, you run 64 Python processes.
Maybe that's just a number in a microwave config file. It's nothing.
Yeah.
It works for applications that are designed to sort of handle multiple independent requests in a scalable fashion.
There are other things that other algorithms that you would want to execute where it's much more complicated to sort of employ all your cores efficiently.
Yeah.
Absolutely.
That's still enough that Python hasn't cracked.
And I'm assuming you're asking this question because Sam Gross, very smart developer at Facebook, claims that he has cracked it.
Perhaps he has.
It's an interesting idea.
We'll dive into that a little bit later.
I'm more asking it just because I see a lot of people say that Python is too slow.
And then I also see a lot of people being very successful with it, and it not being slow in practice or not being much slower than other things. And so I'm more or less at the stage of, like, the context matters.
Right.
This Dropbox example you have it really matters to them.
My course website where people take courses, the response time of the pages is 40 milliseconds. If it was 38, it doesn't matter.
It's really fast. It's fine.
But if I was trying to do computational biology in Python really want to be able to take advantage of the 16 cores.
Right.
So there's just such a variety of perspectives where it matters.
Mark, what are your thoughts on all this?
Well, it's just a case of saving energy saving time.
It just makes the whole thing nice to use.
So there's a lot of just initiative development in data science and that responsiveness, the whole just breaking your train of thought because things take too long versus just keeping in the flow.
It's just nice to have something that's faster.
I mean, it's not just the big companies saving money as well.
It just keeps everyone server budgets down if you just need a smaller virtual instance because you can serve the request fast enough because Python is faster. So just generally it's a responsible thing to do.
So people expect technology to move forward.
It is this feeling of falling behind or people wanting to move other languages because of the perceived performance.
I do think that that's an issue.
I'm moving to go because it has better async support, rewriting this in rust for whatever reason.
Sometimes that might make sense. But other times I feel like that's just a shame.
And it could be better.
A couple of questions from the audience just want to throw out there.
Let's see.
One was especially you must be really proud to hear about the Mars helicopter and the lander and Python in space.
How did you feel when you heard about the helicopter using Python and the lander using Python and Flask and things like that?
It wasn't really a surprise given how popular Python is amongst scientists.
So I didn't throw a party, but it made me feel good. I mean, it's definitely sort of one of those accomplishments for a piece of technology if it's actually shot into space, you know, you've made a difference.
I remember, like, 30 years ago or more when I helped some coding on European project called Amiibak, which was like a little distributed operating system.
And one of the things that they always boasted was that our software runs on the European space station.
That was very important.
Yeah.
I totally get the feeling. And I hope that everyone who contributed to Python also sort of feels that their contribution has made that sense of awe.
You look up in the night sky, that little bright star that's actually Mars, and you think, yeah, it's up there.
Fantastic.
All right.
Let's dive into some of the performance stuff that you all have been doing.
So maybe Guido starts out with the team.
You've got a group of folks working together.
It's not just you. And also now Mark Shannon is working with you as well. Right.
That's correct.
In March or so, the initial team was Eric Snow, Mark and myself.
And since I think since early October, we've got fourth team member, Brand Booker, who is also a Python corded, since I think since about a year and a half, he's a really smart guy.
So now we have four people.
Except you should really discount me as a team member because I spend most of my time in meetings, either with a team or with other things going on at Microsoft in practice.
Sure.
How closely do you work with, say, the Vs code, Python plugin team and other parts? Or is this more a focused effort?
This is more focused.
I know those people I've not met anyone in person.
Of course not.
I've been to Microsoft Office since I started there.
Which is really crazy.
But what we're doing is really quite separate from other sort of Python related projects at Microsoft, but I do get called into meetings to give my opinion or sort of what I know about how the community is feeling or how the core dev team is feeling about various things that are interesting to Microsoft, or sometimes things that management is concerned about.
Excellent.
I'd be worth saying it's not just Microsoft as well. It contributes there's quite a few other core developers are helping out.
It's a broader effort.
This portion of Talk Python to Me is brought to you by Shortcut, formerly known as Clubhouse IO.
Happy with your project management tool.
Most tools are either too simple for a growing engineering team to manage everything or way too complex for anyone to want to use them without constant prodding.
Shortcut is different, though, because it's worse.
Wait, no, I mean it's better.
Shortcut is project management built specifically for software teams. It's fast, intuitive, flexible, powerful, and many other nice, positive adjectives.
Key features include team based workflows.
Individual teams can use default workflows or customize them to match the way they work.
Orgwide goals and roadmaps.
The work in these workflows is automatically tied into larger company goals.
It takes one click to move from a roadmap to a team's work to individual updates and back. Pipe version control integration, whether you use GitHub, GitLab or Bitbucket Clubhouse ties directly into them so you can update progress from the command line. Keyboard friendly interface. The rest of Shortcut is just as friendly as their power bar, allowing you to do virtually anything without touching your mouse.
Throw that thing in the trash. Iteration-planning, set weekly priorities, and let Shortcut run the schedule for you with accompanying burn down charts and other reporting.
Give it a try over at 'Talkpython.fm/shortcut'.
Again, that's 'talkpython.fm/shortcut' choose Shortcut because you shouldn't have to project manage your project management.
Mark, what's your role on the team?
I already have sort of official roles, but I guess I'm sort of doing a fair bit of technical sort of architectural sort of stuff, obviously, because it's just like my field, right.
Optimizer
Yeah, I guess so.
All right. Guido, you gave a talk at the Python Language Summit in May this year talking about faster Python, this team some of the work that you're doing.
So I thought that might be a good place to start the conversation.
Yeah, some of the content there is a little outdated.
Well, you just have to let me know when things have changed.
So one of the questions you ask is, can we make C Python specifically faster?
And I think that's also worth pointing out. Right. There's many run times.
Often they're called interpreters. I prefer to the runtime word because sometimes they compile and they don't interpret sometimes they're called virtual machines.
Yeah, there's many Python virtual machines PyPy C Python.
Traditionally there's been Jython and IronPython, although I don't know if they're doing anything but your focus and your energy is about how do we make the Python people get if they just go to their terminal and type Python the main Python faster? Because that's what people are using, right?
For the most part.
I don't have specific numbers or sources, but I believe that between 95% and 99% of people using Python are using some version of C Python. Hopefully not too many of them are still using Python too.
Yes, I would totally agree with that. And I would think it would trend more towards the 99 and less towards the 95, for sure, maybe a fork of C Python that they've done something weird too. But yeah, I would say CPython.
So you ask the question, can we speed up C Python and Teddy out in the live stream be able to catch this comment?
Exactly.
What will we lose in making Python faster, if anything, for example, what are the trade offs? So you point out, well, can we make it two times faster, ten times faster, and then without breaking anybody's code?
Right.
Because I think we just went through a two to three type of thing that was way more drawn out than I feel like it should have been.
We don't want to reset that again, do we?
No.
Well, obviously the numbers on this slide are just teasers.
Of course, I don't know how to do it.
I think Mark has a plan, but that doesn't necessarily mean he knows how to do it exactly.
Either.
The key thing is and sort of to answer your audience question without breaking on anybody's code. So we're really trying to sort of not have there be any downsides to adopting this new version of Python, which is unusual because definitely if you use PyPy, which is, I think, the only sort of competitor that competes on speed that is still live, and in some use you pay in terms of how well does it work with extension modules?
It does not work with all extension modules, and with some extension modules, it works, but it's slower.
There are various limitations, and that in particular, is something that has kept many similar attempts back.
If we just give this up, we can have X, Y, and Z, right. But those turn out to be pretty big compromises.
Absolutely.
And sometimes quite often extension modules are the issue.
Sometimes there are also things where Python's runtime semantics are not fully specified.
It's not defined by the language when exactly objects are finalized, when they go out of scope.
In practice, there's a lot of code around there that in very subtle ways depends on C Python's finalization semantics based on reference counting and so anything.
And this is also something that PyPy I learned, and I think Piston, which is definitely alive and open source.
You should talk to the Piston guy if you haven't already.
But their first version, which they developed many years ago with Dropbox, suffered from sort of imprecise finalization semantics.
And they found with early tests on the Dropbox server code that there was too much behavior that didn't work, right?
Because objects weren't always finalized at the same time, or sometimes in the same order as they were in standard CPython.
Oh, interesting.
So there's no promises about that, right? It just says, Well, when you're done with it, it goes away pretty much eventually.
If it's a reference account, it might go away quickly.
If it's a cycle, it might go away slower.
That's correct.
And unfortunately, this is one of those unspecified parts of the language where people in practice all depend on not everybody, obviously, but many large production code bases do end up depending on that, not sort of intentionally. It's not that a bunch of application architects got together and said, we're going to depend on precise finalization based on reference counting. It's more that those servers, like the 5 million lines of server code that Dropbox had when I left, were written by hundreds of different engineers, some of whom wrote only one function or two lines of code, some of whom sort of maintained several entire subsystems for years.
But collectively, it's a very large number of people who don't all have the same understanding of how Python works and which part is part of the promises of the language, and which is just sort of how the implementation happens to work.
And some of those are pretty obvious.
I mean, sometimes there are functions where the documentation says, Well, you can use this, but it's not guaranteed that this function exists or that it always behaves the same way.
But the sort of the finalization behavior is pretty implicit.
Yes.
Mark, what are your thoughts here?
People just expectations is derived from what they use.
Trouble, and documentation is like instructions they don't always get red.
And also it's not just the finalization.
It's also reclaiming memory.
So anything that has different memory management system might just need more memory reference counting is pretty good at reclaiming memory quickly and will run near the limit of what you have available.
Whereas a sort of more tracing garbage collection like pipe pie doesn't always work so well like that.
One thing we are going to change is the performance characteristics. Now, that should generally be a good thing. But there may be people on who rely on more consistent performance.
You may end up unearthing race conditions, potentially that no one really knew was there.
But I would not blame you for making Python faster, and people who write bad, poorly threat safe code fall into some trap there. But I guess there's even those kinds of unintended consequences.
I guess that one sounds like pretty low risk.
To be honest.
Also, the warm up time will get a warm up time. Now, what will happen is, of course, it's just getting faster. So it's no slower to start with, but it still has the perception that it now takes a while to get up to speed, whereas previously it used to get up to speed very quickly because it didn't really get up to speed.
It stays around.
It stayed at the same speeds.
But these are subtle things, but they're detectable changes that people may notice.
Like any Optimizer.
There are certain situations where the optimization doesn't really work.
It's not necessarily a customization, but somehow it's not any faster than previous versions, while other similar code may run much faster.
And so you have this strange effect that you make a small tweak to your code, which you think should not affect performance at all. Or you're not aware that suddenly you've made that part of your go 20% slower.
Yeah, it is one of our design goals not to have these pricing performance edges, but, yeah, there are cases where it might actually make a difference.
Things will get a bit slower.
Yeah, there are very subtle things that can have huge performance differences that I think people who are newer to Python run into like, oh, I see you can do this.
Comprehension and I had square brackets, but I saw they had parentheses.
So that's the same thing. Right?
So much, not so much. None of it's a million lines of code or a million lines of data.
All right.
So that's a great way to think about it. Not making it break. A lot of code is, I think as much as it's exciting to think about completely reinventing it, it's super important that we just have a lot of consistency now that we've kind of just moved beyond the Python 2 versus 3 type of thing.
Also, it's worth mentioning Guido you gave a shout out to Sam Gross's proposal.
Stuff you're doing is not Sam Gross's proposal. It's not about even from what I can see from the outside, that much about threading. It's more about how do I make just the fundamental stuff of Python go faster? Is that right?
That's right. These are, like completely different developments.
When we started this, we didn't actually know Sam or that there was anyone who was working on something like that.
But there have been previous attempts to remove the GIL, which is what SEM has done.
And the most recent one of those was by Larry Hastings, who came up with the great name, the Gallectomy.
That's a fantastic name.
Yeah, put a lot of time in it, but in the end, he had to give up because the baseline performance was just significantly slower than Vanilla interpreter.
And I believe it also didn't scale all that. Well, although I don't remember whether it sort of stopped scaling at five or ten or 20 cores, but then claims that he's sort of got the baseline performance.
I think within 10% or so of Vanilla 3.9, which is what he's worked off.
And he also claims that he has a very scalable solution, and he obviously put much more effort in it, much more time in it than Larry ever had.
Yeah.
It sounds like Facebook is putting some effort into funding his work on that, which is great.
Yeah.
But it feels like a very sort of bottom up project.
It feels like Sam thought that this was an interesting challenge, and he sort of convinced himself that he could do it.
And he sort of gradually worked on all the different problems that he encountered on the way.
And he convinced his manager that this was a good use of his time.
It's my theory, because that's usually how these projects go.
But you almost never have management say, oh, we got to find an engineer to make faster or make it multicore or whatever.
Find a good engineer.
It's probably like that Dropbox story you told we have all these servers. There's a lot to maintain.
Hey, what if we could have fewer of them?
What if we could do better?
That's something a manager that could totally get behind.
All right.
So you all are adopting what I see is going as the Shannon plan is in. Mark Shannon, I guess in the top left here.
That's fantastic.
I remember talking about this as well that you had posted this thing.
When was this back?
A little over a year ago. So interesting time in there.
Right.
You had talked about making Python faster over the next four releases by a factor of five, which is pretty awesome.
And you have a concrete plan to sort of make changes along each yearly release to add a little bit of performance because of geometric growth may get quite a bit faster over time.
Yeah.
Do you want to run through these?
Yeah.
Tell us about your plan.
You've got four stages.
Maybe we could talk through each stage and focus on some of the tech there.
The way we're implementing it is now kind of a bit of a jumble of stage one and two. But the basic idea is that dynamic languages, the key performance improvement is always based on specialization.
So obviously, most of the time the code does mostly the same thing as it did last time.
And even in, like, non loopy code for a web server, they're still like a big loop level. That sort of like request response level.
So you're still hitting the same sort of code.
And those codes are doing much the same sort of thing. And the idea is that you multiply the code. So it works for those particular cases.
Specialize in the obvious sort of simple stuff is like binary arithmetic.
I have a special version of adding it to a special version floats.
Obviously, Python, it's much more to do. Special versions for different coding, different things and different attributes and all this sort of stuff that's the key first stage that's mixed in with the second stage, which is really much more to just doing lots and lots of little bits and Tweaks memory layout, so that's to do better memory layout.
Modern CPUs are extremely efficient, but you still have to fetch from Speedline issues with fetching stuff from memory.
So how things are laid out in memory is key performance. And it's just those little bits and tweaks here and just kind of writing the code as we would if it had been written for speed in the first place.
A lot of C Python is old and it's just sort of evolved and a lot of it has this lost potential for just sort of rearranging data structures and rearranging the code and so on.
And these all add up a few percent here, a few percent there. And it doesn't take many of those to get at least speed up.
So that's the first two stages and those are the ones where we have some pretty concrete idea of what we're doing, right.
And this is the kind of stuff that will benefit everybody.
We all use numbers, we all do comparisons, we all do addition call functions and so on.
Yeah. I mean, the way we're sort of trending with performance in the moment is that sort of Webby type code, but web back end sort of code.
You'd be looking at kind of where we are now 25 30% speed up, whereas if it's machine learning sort of numerical code, it's more likely to be 10% region.
Obviously, we'd hope to push both up.
More.
I don't think we're particularly focused on either.
It's just often a case where the next sort of obvious sort of convenient speed up lies.
Although everyone talks about speed ups, I've been doing the same myself.
It's best think of really at the time something takes to execute, so it's often just shaving off 1% of the time rather than being up by 1%. And because obviously, as the overall runtime shrinks, what were marginal improvements become more valuable.
Shaving off 0.2% might be worth it now, but once you spread something up by a factor of three or four, then that suddenly becomes a percent.
And it's worth the efforts.
This portion of Talk Python to Me is sponsored by Linode.
Cut your cloud bills in half with Linode's Linux virtual machines.
Develop, deploy, and scale your modern applications faster and easier.
Whether you're developing a personal project or managing larger workloads, you deserve simple, affordable and accessible cloud computing solutions.
Get started on Linode today with $100 in free credit. For listeners of Talk Python, you can find all the details over at 'talkpython.fm/linode'. Linode has data centers around the world with the same simple and consistent pricing, regardless of location.
Choose the data center that's nearest to you.
You also receive 24/7, 365 human support with no tears or handoffs, regardless of your plan size.
Imagine that real human support for everyone.
You can choose shared or dedicated Compute instances, or you can use $100 in credit on S3 compatible object storage, managed Kubernetes clusters, and more.
If it runs on Linux, it runs on Linode.
Visit 'Talkpython.FM' and click the Create Free Account button to get started.
You can also find the link right in your podcast player showing us.
Thank you to Linode for supporting Talk Python.
Yeah, which leads on to stages three and four. So just in time compilation is always hailed as a way to speed up interpreted languages.
Now, before you move on.
Let me just sort of list out what you have on stage, too, for people who haven't drove into this because I think some of the concrete details people hear this in the abstract. They kind of want to know.
Okay, Well, what actually are some of the things you all are considering. So improved performance for integers less than one machine word.
It's been a long time since I've done C++ is a word. Two bytes. How big is a word word is, how big?
It depends on the machine. So that'll be 64 bits for pretty much anything now, apart from, like, a little tiny embedded system, which is 32.
So that's a lot of numbers.
Many of the numbers you work with are less than 2 billion or whatever that is.
Yeah, basically, there are two types of integers. There are big ones they use for cryptography and other such things, where it's a number in a sort of mathematical sense. But it's really some elaborate code.
And then those numbers actually represent the number of things or the number of times you're going to do something.
And those are all relatively tiny and they are all fit. So the long ones used for cryptography and so on are relatively rare and they're quite expensive.
So it's the other ones we want to optimize for, because when you see an integer that's the integers you get, they aren't in the quarterly.
They're in the thousands.
Right.
Exactly.
A loop index or an array index or something.
Some languages, one that I'm thinking of. That also, maybe is kind of close to where Guido is right now. Also in Microsoft space is C#, which treats integers sometimes as value types and sometimes as reference types, so that when you're doing, like loops and other stuff, they operate more like C++ numbers and less like Py pointers to Py long objects.
Have you considered any of that kind of stuff? Is that what you're thinking?
An obvious thing is an old thing as well is to have tagged integers. So basically, where we would normally have a pointer, we've got a whole bunch of zeros at the end.
64 bits machine is three, and then for alignment, it's often effectively four zeros at the end.
So we're using a 16th of the possible numbers that a pointer could hold four pointers, which means leaves a bunch of integers and floating point numbers.
So there's a number of what's called tagging schemes.
For example, Lua Jet, which is a very fast implementation of Lua, uses what's called nanboxing, which is everything is a floating point.
But there is sophisticated something like two to 53, which is a huge number of not the numbers in the floating point range. So you can use a lot of those for integers or pointers. Now that's a little problematic with 64 bit pointers, because obviously 64 bit 53.
But there are other schemes where you get a simple scheme is that basically the least significant bit is one for pointers and zero for integers or vice versa, and basically just gives you full machine performance for integers, because basically anything up to 63 bits sits in a 64 bit integer, and that's basically all of your numbers.
Interesting.
Okay, Because it's shifted across all the machine. The arithmetic works as normal and overflows.
You just overflow checks and a single machine instruction and things like this.
That's again pretty standard and any sort of like fast Lisp implementation and older small talk and other historical languages.
Javascript tends to use things like this NAN boxing I was talking about because all of the numbers are floating point.
So another one that stands out here is zero overhead exception handling that's making it into 311 already. Right?
That's basically just what we used to have is we'd have a little set up and sort of tear down except instruction for every time we wanted to control block of code inside a try as you try finally, but also with statements, but we've just ditched those in favor of just a table look up. So there's an exception now it's just looked up at the table, which is what the JVM Java virtual machine does.
Yeah. Excellent.
Zero overhead is a slightly optimistic term.
It's obviously not zero overhead, but it is less.
You have a harder time find it in the profiler.
There's a little bit of memory that you didn't have before.
That's a look up table, but sort of it really is zero overhead if no exceptions happen. Right
Not quite just because there is extra memories that causes, but also because of like tracing guarantees.
Sometimes we have to insert a note where the try was.
So there's still some slight overhead then potentially in future when we compile code that should effectively become zero.
But it is definitely reduced.
Mark Apple surprised the world and they took their phone chips and turned them into desktop chips.
And that seemed to actually work pretty well with the Arm stuff.
There's a switch not just having basically just X 86 and 64 bit stuff to think about. But now you also have this Arm stuff. Does that make life harder or easier?
Does it open up possibilities, or is it another thing to deal with?
It's just harder because we were never excluding those anyway, and we may want to look to the future risk five currently, CPython makes Net is affordable.
That's a key thing.
It's portability rather depends on testing.
It's all very well, saying it's perfectly portable, but if you have never tested on a platform, you may have surprises, but it's all written in C.
Portability is a sort of serious consideration.
So things like this tagging. I was just talking about that's technically not portable C, but it's basically a lot of things aren't technically portable C, but in effect, technically it's impossible to write memory allocator and C, because the specification says once you've called free, you can't access the memory, which makes it kind of difficult to write something that handles the memory.
But these are oddities.
But in practice, if you write sensible C Code that you should expect to be portable.
So we are kind of basing around that.
I mean, like some other virtual machines, particularly JavaScript ones are effectively written.
They're interpreted often written in assembler or some variant of it.
There's definitely a performance advantage in that, but I'm not convinced it's great enough to lose the portability and the maintenance overhead.
Yeah, and one of the things that you focused on Guido was that you wanted this to be to keep one of the constraints is you said you want to keep the code maintainable, right.
This is important.
Absolutely.
Why does that matter so much rather than if we can get 20% speed up? If Mark refreshes his assembly language skills.
Well, it would leave most of the core development team behind.
And so suddenly Mark would be very valuable contributor because he's the only one who understands that assembly code. That's just how it goes.
And I don't think that that would be healthy for the Python ecosystem if the technology we used was so hard to understand and so hard to learn, making it so hard to maintain.
Then, as an open source project, we lose velocity.
The only thing that would sort of cause to happen in the core team might be people decide to move more code to Python code, because now the interpreter is faster anyway, so they don't have to write so much in C Code. But then, of course, likely it's actually going to be slower.
At least that particular bit of code.
That's an interesting intention to think about. If you could make the interpreter dramatically faster, you could actually do more Python and less C.
I don't know.
It would have to be there's some big number where that happens, right? It's not just a 10%.
But maybe that could be in the distant future, but Nevertheless, I wouldn't want the C Code to be unreadable for most of the core developers.
Yeah, I agree.
That makes a lot of sense.
Being a C expert is not a requirement for being a core developer. In practice, quite a few of the core developers are really good C coders, and we support each other in that we take pride in it, and we help each other out.
Code reviews are incredibly important, and we will happily help newbies to sort of get up to speed with C if we had a considerable portion that was written in assembler, and then it would have to be written in multiple assemblers.
Or there would also have to be a C version.
For platforms where we don't have access to the assembler.
Nobody has bothered to write that assembler code yet.
All these things make things even more complicated than they already are.
Right, and the portability the approachability of it is certainly a huge benefit.
Two other constraints that you had here, maybe you could just elaborate on real quick is don't break stable ABI compatibility, and don't break limited API compatibility.
Yeah, so the stable ABI is the Application Binary Interface, and that guarantees that extension modules that use a limited set of Capi functions don't have to be recompiled for each new Python version.
And so you can, in theory, have a wheel containing binary code, and that binary code will still be platform specific, but it won't be Python version specific.
Yes, that's very nice.
We don't want to break that. It is a terrible constraint because it means we can't move fields like the reference count or the type field around any object, many other things as well, but nevertheless, it is an important property because people depend on that.
Sure and the API compatibility. Well, that's pretty clear. You don't want people to have to rewrite it.
The limited API is sort of the compiled time version of the stable Abi. I think it's the same set of functions, except the stable Abi actually means that you don't have to recompile. The limited API offers the same.
And I think a slightly larger set of API functions where if you do recompile, you're guaranteed to get the same behavior.
And again, our API is pretty large, and a few things have snuck into the limited API and stable ABI that sort of are actually difficult to support with changes that we want to make.
And so sometimes this holds us back. But at the same time, we don't want to break the promises that were made to the Python community about API compatibility.
We don't want to say sorry, folks, we made everything 20% faster, but Alas, you're going to have to use a new API, and all your extensions just recompiling isn't going to be good enough.
Some functions suddenly have three arguments instead of two or no longer exists or return memory that you own instead of returning a borrowed reference.
And we don't want to do any of those things because that just would break the entire ecosystem in a way that would be as bad as the Python 3 transition, right.
Yeah.
It's not worth it.
All right.
Let's go back to the Shannon plan. So we talked about stage one and stage two and Mark, I see here. This is Python 3.10 and Python 3.11.
Are those the numbers where they're actually going to make it in? Or do we just do, like, a plus plus or plus SQL on them?
I think the plus one would be appropriate.
All right. Plus equals one.
Yeah. So maybe a bit faster because obviously I envision this with basically me and one other developer plus, maybe sort of some sort of reasonable buy in from the wider code of a team.
So I wasn't sort of doing the work entirely in isolation, but yeah, I'm still having extra hands will definitely help things.
So back when you were thinking this was written at 39 timeframe, right. And you're like, okay, well, the next version, maybe we can do this. The version after that. And by the time it really got going, it's more like 311, 312 and so on right
Yeah It's just around the time. I think we switched from 3.09 to 3.10.
Okay. So stage three out of the four stages you have is, I guess, Python 313 now, which is a miniature jet compiler.
Is that right characterization.
I think that's not the compiler.
Well, I suppose it will be smaller.
Maybe the parts it applies to the parts that get.
Yeah so I think the idea is that you want to compile all of the code where performance matters that sort of hot code, but it makes life easier if you just compile little chunks of code and sort of stitch them together afterwards, because it's very easy to fall back into the interpreter to jump into sort of compiled code.
You can sort of just hang these bits of compiled code off by individual byte codes where they sort of start from.
Obviously, that's not fantastic for performance, because you're having to fall back into interpreter, which limits your ability to infer things about the state of things. So obviously, as I said earlier, specialization, you have to do some type checks and other sort of checks.
If you've done a whole bunch of checks, if you then fall back into the interpreter, you have to throw away all the information.
If you compile a bigger region of code, which is of the stage four, then you already know something about the code and you can apply those compilations.
The problem with trying to do big regions up front is that if you choose poorly, you can make performance worse.
And this is the real issue for existing ones. I think we're going to talk about some of the other historical sort of compilers in the past, and this is a real issue for those that they just try to compile method at a time, regardless of whether that is a sensible unit to compile, right.
It's sometimes hard to do optimizations when it's too small, right.
Yeah and also it's very expensive to do regions that are too big or just in the bounded in the wrong places.
Okay, yeah, that definitely sounds tricky.
There was a question earlier about MyPy C work and the mypy stuff, and you are really central to that doing a lot of work there.
How do both of you either of you feel about using type annotations as some sort of guide to this compiler? For example, Cython? Let's just say X: int as a parameter, and it will take that as meaning something.
When you compile with Cython, it seems like Mark is talking about knowing the types and guessing them correctly matters in terms of what's faster is there any thought or appetite for using type annotations to mean more than static analysis?
It's a great idea.
And I think for smaller code bases, something like mypy C will prove to be viable or for code bases where there is an incredible motivation to make this happen.
I could see that happen at Instagram, for example.
But in general, most people haven't annotated their code completely and correctly.
And so if you were to switch to using something like mypy C, you'd find that basically it wouldn't work a large number of cases.
It's a different language and it has different semantics, and it has sort of different rules.
And so you have to write to that.
I can see there's a big challenge to say, hey, everybody, we can do this great stuff. If you type annotated, and only 4% of people have properly annotated their code, and then there's also the possibility that it's incorrectly annotated, in which case it probably makes it worse in some way of a crash or something.
Mypy C will generally crash if a type is detected.
That doesn't match the annotation.
Yeah, and if you annotate stuff with simple types, you can get quite good speed up. So number is generally time for numerical stuff. But again, it's a simple type. Integers, floats Cython do this number, does it dynamically side statically.
And the number model, for example, is similar to the model that Julia language users.
Essentially, you compile method at a time, but you make as many specializations as you need for the particular types.
And I can give very good performance for that sort of numerical code.
But the problem is that saying something is a particular type doesn't tell you very much about it.
It doesn't tell you what attributes an instance of it may or may not have.
It depends.
It's not like Java or C++ where having a particular class means it has those instance attributes and they will exist, or at least they exist in a particular place, and they can be checked very efficiently because of dictionary look up and so on. These things get a bit fuzzy, sir.
72 bytes into the C object is where you find the name or something like that. Right? Yeah.
So basically because anything might not be, as the annotations say, effectively at the virtual machine level, we have to check everything.
And if we're going to check it anyway, we may as well just check it once up ahead as we first do the compilation or ever specialization, and then assume it's going to be like that, because if the annotations are correct, then that's just as efficient.
And if the annotations are wrong, we still get some performance benefit, and it's robust as well. So there's really no.
The only advantage annotations is for this sort of like very sort of loopy code where we can do things like loop transformations and so on, because we can infer the types from the arguments of enough of the function to do stuff. And that works great for numerical stuff, but for more general, code is problematic.
What about slots?
Slots are an interesting, not frequently used aspect of Python types that seem to change how things are laid out a little bit.
One of Mypy C main tricks is that it turns every class into a class with slots.
If you know how slots work, you will immediately see the limitation because it means there are no dynamic attributes at all.
Yeah.
These are what you get for your fields, and that's it. Yeah.
I mean, if you don't have dynamic attributes, though, it gives you pretty efficient memory use.
It's not too far of Java and more predictability about what's there and what's not, which is why they came to mind.
Yeah, they definitely have their use.
All right, Mark, that was your four stage plan, hoping to make 1.5 times as fast as before.
Each time which you do that over four releases, you end up with five times faster. Right. That's the Shannon plan.
Where are we on this?
How's it going for you and everyone on the team?
I say it's a bit of a jumble of stages one and two that we're implementing largely because it's a larger and more diverse team that was expecting. So it makes sense to just sort of spread things.
You'll work on operators, you go work on zero overhead, accept handling.
Yeah I would say from where we are now, I was probably a bit optimistic with stage one, but stage two seems to have a lot of potential.
Still, there's always little bits of the interpreter we can tweak and improve.
So between the two of them, I'm confident we'll get projected over twice the speed.
That's fantastic. So the course you're on right now if let's just say stage one and two happen, and for some reason, the jet stuff doesn't.
That's still a big contribution.
What do you think in terms of speed up for that?
Well, again, it's going to depend a lot.
I know it matters so much.
But I just want to because currently we have a sort of set of benchmarks that we're using.
The more benchmarks is always better. So it's a broad set individually, the benchmarks, some of them are great, but collectively, it's a sort of useful, they said. But I mean, we speed up from up, like up to 60%, down to zero so it's definitely a spread, so it can try it out would be the thing. I mean, you can download 311, Alpha One and Alpha two should be out a few days at all time now, presumably before they publish a podcast.
Yeah. Fanstatic
So people can download it play.
Yeah, that's fantastic.
Thank you for this. I think even 50 60% if it stays there.
That's pretty incredible.
This language has been around for 30 years.
People have been trying to optimize it for a long time. It's incredible. Right. And then to do this sort of change now, that would be really significant.
Yeah This is an area that we haven't spent much time on previously for various reasons.
People have spent a lot of time on sort of making the string objects fast, making dictionary operations fast, making the memory efficient, adding functionality that the Python has generally, I think, has more of a focus on functionality than on speed.
And so for me, this is also a change in mindset.
I'm still learning a lot.
Mark actually teaches me a lot about how to think about this stuff, and I decided to buy this horrible book.
Well, it's great book, but it weighs more than 17 laptop.
Classic text, but not a light read.
Yeah down into beyond the software layer into the hardware beds.
It makes me amazed that we have any performance at all, and that any performance is predictable because we're doing everything wrong from the perspective of giving the CPU something to work with.
All the algorithms described in their branch prediction, speculative execution, caching of instructions, all that is aimed at small loops of numerical code, and we have none of that.
Yeah, exactly.
C else is not a numerical loop.
Definitely not.
All right. Well, I think that might be it for the time we have.
I got a couple questions from the audience out there to be. An army captain says I'm interested in Guido's thoughts about the Microsoft funded effort versus the developer in residence, particularly in terms of the major work of the language and the C Python runtime going forward.
I think these are both good things, both really good things.
They seem super different to me.
I think it's great that we have a developer in residence.
It's a very different role than what we're doing here.
The team at Microsoft is at least we're trying to be super focused on performance to the exclusion of almost everything else except all those constraints I mentioned.
Of course, the developer in residence is focused on sort of the community other core developers, but also contributors.
Lucas is great.
He's the perfect guy for that role, and his work is completely orthogonal to what we're doing.
I hope that somehow the Psf finds funds for keeping the developer in Residence role and maybe even expanding it for many years.
It seems to me like a really important role to smooth the edges of people contributing to C Python and the difference of what Mark and you all are doing is heads down focused on writing one type of code, whereas Lucas is there to make it easier for everyone else to do whatever they were going to do.
And I think one sort of a horizontal scale of the C Python team, and the other is very focused, which is also needed.
It's actually amazing that we've been able to do all the work that we've been doing over the past 30 years on Python without a developer in residence.
I think in the early years, I was probably taking up that role, but the last decade or two, there just have been too many issues, too many PEPs for me to sort of get everything going.
I was always working part time on Python and part time working on my day job.
Right. Absolutely.
Lucas is working full time on Python, and he has a somewhat specific mandate to sort of.
Help people.
Contributions go smoother.
Make working with the issue tracker easier.
And that sort of developer contributors must be encouraged and rewarded.
And currently often the way the Python.
Org experience is.
It's a very old web app, and it looks that way and it's difficult to learn how to do various things with that thing.
And so Lucas is really helping people.
Yeah, it's fantastic.
Of course.
There's also the somewhat separate project of switching from Bugs Python.
Org to a purely GitHub based tracker.
Yeah I was just thinking of that as you were speaking there. Do you think that'll help? I feel like people are more familiar with that workflow.
People are more familiar.
It's more integrated with the pull request flow that we already have on GitHub.
I think it'll be great expectations is that I think it'll be actually happening before the end of this year or very early next year.
That would be fantastic.
The code is already there. The work is already there.
Might as well have the conversations and the issues and whatnot I think we are definitely overtime, but I really appreciate, first of all, the work that you're doing, Mark on this project Guido on the last 30 years. This is amazing. You can see on the comments how appreciative folks are for all the work you've done. So thank you for that.
Let's close with a final call to action.
You have the small team working on. I'm sure the community can help in some way.
What do you want from people?
How can they help you either now or in the future?
I mean, it's just contribute to C Python, so I don't think it's specifically performance.
All contributions help improve code quality and reliability are still very important.
So I don't think two, three people can do, but we do have ideas repo. If people do have things they want to suggest or bounce ideas around.
Maybe they could test their workloads on Alpha versions of things like that.
Yeah, that would be fantastic.
We don't really have a set for where people can put information, but just open an issue on the ideas thing and have some data.
Be fantastic.
We'd love it for people to try to use the new code and see how it works out for them.
Yeah, fantastic.
All right. Well, thank you both for being here. It's been great.
Our pleasure.
Thank you.
This has been another episode of Talk Python to me.
Thank you to our sponsors.
Be sure to check out what they're offering. It really helps support the show.
Choose Shortcut formerly Clubhouse IO for tracking all of your projects work because you shouldn't have to project manage your project management.
Visit 'talkpython.fm/Shortcut' Simplify your infrastructure and cut your Cloud bills in half with Linode's Linux virtual machines.
Develop, deploy and scale your modern applications faster and easier.
Visit 'TalkPython.fm/linode' and click the Create Free Account button to get started.
Do you need a great automatic speech to text API?
Get human level accuracy in just a few lines of code?
Visit 'talkpython.fm/assemblyAI' want you level up your Python, we have one of the largest catalogs of Python video courses over at Talk Python.
Our content ranges from true beginners to deeply advanced topics like memory and async and best of all, there's not a subscription in site.
Check it out for yourself at
'training.talkpython.fm' Be sure to subscribe to the show.
Open your favorite podcast app and search for Python.
We should be right at the top.
You can also find the itunes feed at /itunes, the Google Play feed at /Play and the Direct RSS feed at /rsson Talk Python FM.
We're live streaming most of our recordings these days.
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at 'talkpython.fm/youtube'.
This is your host, Michael Kennedy.
Thanks so much for listening. I really appreciate it.
Now get out there and write some Python code.
