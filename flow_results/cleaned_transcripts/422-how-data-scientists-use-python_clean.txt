Regardless of which side of Python you sit on, software developer or data scientist, you surely know that data scientists and software devs seem to have different styles and priorities.
Why is that? And what are the benefits as well as the pitfalls of this separation?
That's the topic of this conversation with our guest, Dr. Jody Burchill, data science developer advocate at JetBrains.
This is Talk Python to Me, episode 422, recorded May 31st, 2023.
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.
Be careful with impersonating accounts on other instances.
There are many.
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.
We've started streaming most of our episodes live on YouTube.
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.
This episode is brought to you by JetBrains, who encourage you to get work done with PyCharm.
Download your free trial of PyCharm Professional at talkbython.fm/done-with-pycharm.
And it's brought to you by Prodigy from Explosion AI.
Spend better time with your data and build better ML-based applications with Prodigy, a radically efficient data annotation tool.
Get it at talkbython.fm/prodigy and use our code TALKBYTHON, all caps, to save 25% off a personal license.
Jodi, welcome to TalkBythonomy.
- Thank you, I am so thrilled to be on the show.
- I'm so thrilled to have you on the show.
I've been a fan of your work for a while and we got a chance to get to know each other at this year's PyCon.
And so here you are on the podcast as well.
- Thank you, we had some very nice Mexican food actually, or maybe Utah Mexican.
I don't know how I would interpret it.
It was very good though.
- It was very good, yeah.
The food was excellent.
I thought the parties were great at the conference and people who are maybe still holding out ongoing. Personally, I really enjoyed being there.
I think it's probably the best conference that I go to yearly.
And it's like the vibe is so nice.
On this show, we're going to talk about how data scientists use Python, which is somewhat different than maybe a software developer.
We have, which I guess I'll put myself solidly into that, that camp.
I do a bunch of web development, make APIs, I build apps and ship them.
That's quite a different story.
And I'm going to have a, I think we're have a really great time talking about those things.
But before we do, let's get a little, get to know you a bit.
Let's see, how'd you get into programming, Python, data science?
Yeah.
So, probably going hand in hand with maybe not being a developer.
My story is perhaps a little unconventional.
So my background is academic, like a lot of data scientists.
And unsurprisingly, the first language that I learned was R because I was doing psychology and health sciences and a lot of statistics.
And I was procrastinating once during my PhD.
You will find any excuse to not work on your thesis.
And I think I was reading, oh, you know, people who are into statistics, you should really learn Python, it's the future.
And I was like, I should learn Python.
So I sat down and-
- Because I really don't wanna write that next chapter.
I just don't. - Exactly, exactly.
So I remember it, like I actually had this long weekend and I worked my way through, I think it was Zed Shaw's "Learn Python the Hard Way." This is showing my age, I think.
I loved it. Like I completed the course in three days. And then I didn't know what to do with Python because the stats libraries weren't as developed back then. So I just put it aside for a couple of years and ended up picking it up again when I started working in industry, because obviously I've left academia. And you sort of fairly quickly, once you start in data science, move away from more sort of statistical stuff to machine learning.
And Python really has the libraries for that. So that's my journey. It's a little bit bibs and bobs and stops and starts.
But once I kind of picked up Python, it really was love at first sight.
Oh, that's that's excellent.
What's your PhD in?
It's actually computer science, of course.
Right.
Of course.
Of course.
You know, it's so funny.
You are the third person to ask me in two weeks.
And no one has asked me this question for like two years.
My PhD was in hurt feelings.
Hurt feelings?
Yeah.
OK.
I say this a little bit blithely.
So my PhD being in psychology, I was really interested in emotions research and relationships research.
So, I kind of wanted to see what happens to people emotionally when close relationships go bad.
And it's hurt feelings, like things like, you know, infidelities, rejections, all of that.
It's hurt.
So, I was just studying what generates like and regulates the intensity of hurt and studied that for four and a half years.
- Yeah, sounds interesting.
I'm sure there was a lot of data to process.
There was a lot of data to process and a lot of very interesting statistic.
That was sort of how I got into data science.
I fell in love with stats in undergrad and just kept going down that path.
- I think a lot of people are drawn to data science not with the intent of waking up one day and saying, "I'm gonna be a data scientist," but they're excited or inspired about something tangential and they're like, "Well, I really need to get something better than Excel to work on this." Right? - Absolutely.
Yeah, yeah.
And we'll probably talk about this a little bit later about why data scientists use programming.
And it kind of is like, in some ways that need to jump from something way more powerful and reproducible than Excel.
Yeah, yeah, for sure.
So how about now?
You said you've left academics.
And what are you doing these days?
Yeah, so that leap from academia was a long time ago now, I think that was like seven years again, showing my age.
So for six of those years, I was a data scientist.
So day to day was, you know, pretty varied, but the job I have now is very different.
So I currently work as a developer advocate at JetBrains.
And the way I would describe my job is I'm a liaison between data scientists and JetBrains.
So I try and advocate for our tools to be as good as they can be.
And I try to recommend ways that people can use our tools if I think it's useful.
But I'm definitely not marketing or sales.
more if I think this is the right fit for you, I'll do it.
So it's like the way I achieve that is really up to me.
For me, I really kind of I like to do a mixture of what I call internal and external activities. So external activities are actually kind of only tangentially related to the products. So this would be an example of an external activity.
It's just getting out there and educating people about data science or educating data scientists about technical topics, things like conference talks or webinars, you know, all this sort of stuff. And then internal stuff is more focused on maybe things a bit more related to the product. So if I think there's a feature that people would be really interested in, I might make a video about it or create, you know, a blog post. So yeah, it's a real hodgepodge. So this week, for example, I've been working on actually materials for a free workshop that they're organizing at EuroPython. So I'm going to be volunteering to help out that. It's completely unrelated to anything I'm doing at JetBrains. It's just a volunteer activity.
But last week I was at a conference week before that.
So I can see the jobs pretty varied.
I think developer evangelists, it seems like such a fun job.
You know, I had your colleague Paul Everett on and we actually talked.
It's quite a while ago, a couple of years ago, three, four.
And we had a whole episode on like a panel on what is the developer advocate, developer relations job.
But it's it just seems like such a great mix of you still get to travel a little bit, see people, but you also get to write code and work on influencing technology and products and stuff.
Yeah.
And I think the thing that I started to appreciate about the job a few months in is you have a platform with this job, and that means you can choose to promote the message that you want.
And a message that, no surprise, is very meaningful for me is data science is for everyone.
Like I hate the gatekeeping that can happen in tech communities.
I think it's quite bad in terms of like people being intimidated by math in data science.
And like, I'm here to say to you, if you want it, it's for you.
It is a very cool field.
And yeah, doesn't matter what background you come from.
I absolutely agree.
And I was kind of hinting at that saying like a lot of people who don't see themselves as developers or programmers, like still find really great places, really great fits in data science and in programming as well.
And I also want to second that I don't think you really need that much math.
Maybe if you're trying to build the next machine learning model platform, then yes, okay.
But that's not what most people do.
They take the data, they clean it up, they do interesting visualizations and maybe put it into some framework for production, right?
Yeah.
And the nice thing is the field is in such a point where you have so many frameworks or tools that will handle a lot of this stuff for you.
Like, I'm not saying you don't need any understanding of what's going on under the hood, but you can learn it incrementally.
A lot of it is like with software development, where you develop that, that smell or that instinct for when something is not right, that will benefit you more than, you know, knowing how backpropagation works from a calculus perspective.
Like that stuff is maybe a bit too much.
You don't necessarily need it.
Yeah.
Let's get into the main topic and talk a little bit about how does programming in Python differ on the data science side than say me as somebody who builds web apps.
Yeah. And maybe we can start by doing an orientation to like, what does a data scientist do? Because I think this confuses a lot of people. Yeah.
Yeah. So basically, the role of a data scientist is to sort of like, like the reason you would hire a data scientist is you have a bunch of data and you have an instinct that you can use that data to either improve your internal processes or sell some sort of IP.
So the reason, you know, we differ from BI analysts is BI analysts are doing analysis, but it's more about business as usual, which is really important.
You absolutely need BI analysts.
How are sales going?
How much have we made versus last year?
Like those kinds of charts, right?
Like absolutely fundamental questions.
So you need to be an analyst before you need a data scientist, but your data scientists are more there to push the envelope in a data driven way.
So we have two main outputs, I would say.
You can either create an analysis and do a report, or you can build some sort of model that will go into production.
So an example might be as a business, I have an instinct that I can get my customers to buy more things based on people like them also buying those things.
So in that case, your data scientists might be able to build you a recommendation engine. And this will have a business outcome.
Developers, obviously, on the other hand, have a very different goal.
Their goal is to create robust software systems.
So the concerns that they have are things like latency, server load, downtime, things like that.
And it's very interesting.
And we'll talk about it a bit more when we talk about code, if we kind of get into that topic. But basically, data scientists are not really interested in creating code for the long term, whereas the code becomes the product that software developers write often.
And you have to think about things like legacy systems, because eventually every Greenfield project becomes a legacy system if it lasts for long enough.
- Yeah, if you're lucky, right?
Because the alternative is it never really got used or it didn't add that much value and got discarded, shut down, all right.
So even though people talk about how much they don't want legacy code or how they kind of don't necessarily want to work on it 'cause they want to work on something new and shiny, That's kind of the success side of software development, right?
Yeah, exactly.
I do think it's super interesting, the different lifecycle of code on the data science side, because you might be just looking to explore a concept or understand an idea better and not necessarily ever intend to put it into production in the traditional software sense, right?
So I've seen some pretty interesting code written that people would look at and go, oh my goodness, what is, there's not even a function here.
How is this possible? You know, it's like copy and paste, reuse almost. And yet it really does go from having no idea to pictures and understanding and then maybe handing that off potentially to be written more robustly or better. Exactly. And it's really interesting. Like, They're kind of two very different processes. And that point actually where engineering and research meets is a very, very interesting one. And I've seen it work in multiple different ways in multiple different workplaces. So, for example, I've worked in places where the data scientists were completely sequestered away from the engineers. And there really wouldn't be that much discussion between the engineers and the data scientists during the research phase, which I do not advise. So what that means is the data scientists will come at the end and hand over this chunk of perhaps very difficult to read code to an engineer and be like, "Hey, so we need to implement this." And the engineer is like, "What is this? Okay, I will schedule that for the next six months." And then I've seen, or I've been a data scientist embedded within a software development team. And in that case, your project is marching in lockstep with what the the engineers are doing. And from the very start, you know, you've been discussing important things like you need to build a model that has latency constraints. You need to think about this as the data scientist in terms of like the model that you run, but also how it's implemented.
Right. Like how much memory does it use? Because if you run it on your own machine by yourself, then, well, it's kind of the limit of your computer sort of sometimes. But if you're running a thousand of them concurrently, because people are interacting at the website, all of a sudden that might make a difference.
Or like one of the most interesting problems I think I ever solved in my career was basically I was working at a job board.
We're trying to improve the search using natural language processing.
So we had this idea that we could build a model that found out the probabilistic relations between skills and job titles.
So if someone typed a skill into the search, we could expand it with job titles and then find all of the jobs that we indexed with that at search time and vice versa with job titles and skills.
The thing is, we need to find those relations at search time.
That is a very low latency system.
And it was super interesting because we had to think about how we could search that vector space in really, really quickly.
Instead of having to calculate the distance between that and every single vector, we had to work out how to do that more efficiently.
That sort of stuff I really like because it's so applied and it's so...
It's this really nice intersection between computer science and data science, I think.
It's super cool.
One of the things I really like about working with programming broadly is how concrete it is.
Right? You came from academics. You know, I was in grad school for a while as well.
And it's, you could debate on and on about a certain idea or concept. And it's like, well, you might be right. Or I'm here, you push a button and you get the answer or it runs or like, there's a really nice feedback of like, I built this thing and it's look, it's really really connecting these people and you know, then it comes down to can you do it in real time and other things like that. But that's a really cool aspect of programming.
This portion of talk Python to me is brought to you by JetBrains and PyCharm. Are you a data scientist or a web developer looking to take your projects to the next level? Well, I have the perfect tool for you. PyCharm. PyCharm is a powerful integrated development environment that empowers developers and data scientists like us to write clean and efficient code with ease.
Whether you're analyzing complex datasets or building dynamic web applications, PyCharm has got you covered.
With its intuitive interface and robust features, you can boost your productivity and bring your ideas to life faster than ever before.
For data scientists, PyCharm offers seamless integration with popular libraries like NumPy, Pandas, and Matplotlib.
You can explore, visualize, and manipulate data effortlessly, unlocking valuable insights with just a few lines of code.
And for us web developers, PyCharm provides a rich set of tools to streamline your workflow.
From intelligent code completion to advanced debugging capabilities, PyCharm helps you write clean, scalable code that powers stunning web applications.
Plus, PyCharm's support for popular frameworks like Django, FastAPI, and React make it a breeze to build and deploy your web projects.
It's time to say goodbye to tedious configuration and hello to rapid development.
But wait, there's more.
With PyCharm, you get even more advanced features like remote development, database integration, and version control, ensuring your projects stay organized and secure.
So whether you're diving into data science or shaping the future of the web, PyCharm is your go-to tool.
Join me and try PyCharm today.
Just visit talkpython.fm/done-with-pycharm, links in your show notes, and experience the power of PyCharm first hand for three months free. PyCharm, it's how I get work done.
I think it's kind of a shame that a lot of places do set up their engineering and data science teams so separately. Sure, we have quite different roles and we have quite different backgrounds sometimes. But I really think that having the two teams at least planning things together, you can really actually learn a lot from each other about how to approach problems.
When you were describing, you know, either having those groups really separated or working really closely together, maybe an analogous relationship that people could relate with is maybe front-end developers and people building the APIs in the back-end, right?
Like the people doing React or Angular or Vue or whatever it is, you know, in the web design.
Having those completely separated as well is also, you know, it's terrible, not a good idea.
- It doesn't make any sense.
And like, I can totally understand it from the point of view of like team composition, because it is, I think, better to have all your data scientists together because they can learn from each other.
But then I think having, I don't want to use the squad term 'cause I know it's become a little bit unpopular to use it, but you know, this idea of project-oriented teams, I think are quite important.
- Let's dive a little bit more into the research side of things that I want to ask you about.
Why Python?
Let's talk about how the research process works and maybe why that results in different priorities and styles of code and styles of engineering.
It starts at a similar point to all software projects, which is business comes to you and they have some sort of goal. Sometimes it's very vague and you need to interpret that and turn it into an executable project. But where the sort of uncertainty starts and like where it sort of becomes a research project rather than a project, and I don't know if I described that very well, but it becomes research versus something you're building concretely is even know at the start of a research project, whether it's even possible to answer the question that you're being asked or build the internal product that you're being asked for.
You might not understand the domain entirely, right? You're trying to gain understanding even.
In the very worst cases, you won't even know if you have the data because maybe your company has so much data and it's so poorly organized, again, something I've seen that you don't even know if the data exists to answer this question. So, first is going and getting your data. And you spend quite a lot of time with the data because the data will be the one that tells you the story. It'll tell you whether what you even want is possible. And you probably like heard data scientists hammering on about, you know, garbage in, garbage out. Like, you can build the most beautiful, sophisticated model you want. But if you have crap data where there's no signal, you're not going to get anything because it's just not there. Like, the relationship you're you're looking for is not there.
- Yeah, the side of that I've heard is 80% of the work is actually the data cleanup, data wrangling, data gathering before you just magically hit it with a plot or something, right?
- Absolutely, and it's interesting because that data cleaning, data wrangling step also doesn't happen in one go, especially if you're building a model.
So what will happen is you'll try something out and you'll be like, okay, it didn't quite work.
Maybe I need to manipulate the data in a different way or I need to create this new variable then you'll go again. And it's this super iterative process where you have this tightly coupled relationship between both the models and the data. So, it really is sort of, you know, how I was talking about the instinct, this is sort of where that comes in, because you're going to spend like 80% of your time honing your skills. But it's the most, I think, valuable part of the process.
And if the signal's there, you can usually get away with using really dumb and simple models, you know, things that are unfashionable now, like decision trees or linear regression, you can get away with them because you've just got such good data, but just go with a simpler model. It's got all the advantages. This is sort of, I think, what makes it different that you're sort of moving towards a goal, but you don't know what that goal is.
Estimation is always hard, right? What I found is best is really just time boxing each step, seeing if you are up to where you thought you'd be up to by a certain point. And if not, you need to just keep having those discussions with the business stakeholders because otherwise they're going to not be very happy if you've spent six months just looking at something and you have nothing. What have you built? Well, I have some notebooks I could show you. I have 40,000 notebooks and they're all terrible. Yeah. Speaking of the data, Diego in the audience has an interesting question. How big are the data sets businesses will bring to you typically?
enough that you don't need to go out and find more data?
This is a good question. So I hate to be it depends, you know, I get to say that, though, because, you know, I was a lead data scientist. So I earned that, that rank, it really does depend on the problem you need to solve. So typically, business will have enough data to cover at least some of the use cases. So to give you a really concrete example, this job board that I was talking about that I worked at, we actually had like a bunch of different job boards across Europe. So we had some that were a lot bigger, like Germany or the UK, and we had some that were really small, like Poland or Spain. And we wanted to build these multi-language models or models maybe for different languages.
We played around with both. And I don't think we really had enough data to support the models in these smaller languages.
So the models were just not as good quality because we didn't have enough data.
But for the bigger languages we did.
And then it sort of becomes a case of, OK, well, we have more data for these particular websites because they're the most like they're the ones that are bringing the most revenue. So then it sort of became like, well, OK, maybe it's good enough that we improve the search on the most important ones.
And for the smaller ones, we just wait until we accumulate more data.
So, yeah, most of the time I found that there's a way to make it work for at least part of the solution.
And then sometimes, like in the case of my last job, we had something like 170 billion auctions per day.
Sorry.
So we had so much data.
We even had problems like processing it.
So sometimes, you know, that's the other side of the story is when you've got too much and then how do you throw it away?
Right.
I mean, you've got this auction story.
Like another one that comes to mind is the large Hadron collider.
Oh yes.
Yes.
They've got layers and layers of like chips on hardware and then chips or machines right next to the Collectors and then on it where it's all about. How do we throw away?
You know terabytes of data down to get it to megabytes per second, right?
Yeah, and it's interesting because what you can end up in this within those situations is even then you can have underrepresented groups so for example, we had we're working with advertisers and apps, you know, basically trading ads and we ended up with some apps that were just so small that you were like, "Even with all this data, I really don't have enough to represent this particular combination in this country." >> Interesting. Very interesting.
Why Python? You started out in R, and of course, any distraction from writing a PhD is a good distraction.
But I do think there's been a really interesting graph.
If people go and look at, what is it, Stack Overflow Insights.
If you go look at Stack Overflow Insights, they had a really great graph that shows you the popularity of Python over time.
And there's just this huge inflection point around 2012.
And I feel like that's when a lot of the data science libraries really came around and took off.
It seems like there was a big inflection at one point, but, you know, why?
To be honest, I can talk about why I like Python from my background.
I couldn't really tell you exactly what caused that takeoff, but, you know, apart from, you know, this idea that the libraries were maturing enough. But the thing is, looking even at current surveys, around 60% of data scientists do not have a software development or a software engineering background. So, for people like us, we don't really understand, like, it sounds terrible, we don't really understand basic constructs in how a programming language works. And that can actually mean that going to some sort of compiled language even can be quite a steep learning curve.
Sure. Pointers to pointers, for example. Like, "Oh, no thanks." Yeah. Or having to deal with the fact that in Java, everything is a class. You're just like, "What is this?" But of course, you understand why if you have that background. But if you're trying to learn it yourself, you then have a lot of background you need to cover. But in Python, and in R as well, you don't need to cover those things.
It's super easy to prototype, it's super easy to script.
The flexibility of Python is what makes it, I think, the perfect prototyping language.
And that's essentially what you're doing, you're prototyping.
So we talked a little bit earlier about like, why not just Excel?
We didn't quite say that, but this was sort of what we were maybe getting at.
And yeah, we could do some of our work in Excel.
I've tried this.
And first I can tell you Excel really starts to struggle when you have too many calculations going on under the hood, it gets very, very slow. But to be honest, it's sort of just, it's just cleaner to code this sort of logic.
It's much more reproducible when you need to do this iterative sort of stuff.
And it also means that you can use much more powerful tools.
So you can say, use APIs that developers have made to process your data. You can use powerful.
- Or get data, right? Like I need live, live currency conversion data, Right. So much easier than in Excel.
Yes, yes, yes. Exactly.
Like you can like scrape data or you can, yeah, pull data in of an API or you can use powerful tools like Spark to process 170 billion auctions per day in order to reduce it down to something manageable.
So, yeah, it just gives you a lot more power.
But at the same time, why we use programming languages is it's just such a different focus.
it's a bit overkill to use something like Java.
I know some people do do natural language processing in Java, but that's more on the engineering side to build maintainable systems.
>> One of the things I like to say when thinking about how people who are coming from tangential interests like biology or whatever, is you can be really effective with Python and I suspect R as well, with a really partial understanding of what Python is and what it does.
You pointed out you don't even have to know what a class is or even really how to create a function.
You just, I can put these six magical incantations in a file and then I can do way more than otherwise could, right?
Then you learn one more, you make it better and better as you kind of gain experience.
Pretty much, and this is where I started, like, obviously I learned what functions and classes were when I first started programming.
But in the end, you will just, maybe it's not the best thing and we can sort of maybe get into this.
I suppose part of the confusion or not confusion, but internal debate I've had over the years is how good does data science code really need to be? Like, how much would data scientists benefit from knowing more about computer science topics or software engineering topics, maybe more to the point? And, you know, because like the thing is, every field has so much to learn.
Don't even get started on what's happening with large language models at the moment. Like, it's just overwhelming. Should we take some of our precious time and learn software engineering concepts. I'm not sure. Like, I'm not sure if I have the answer to that.
This portion of Talk Python to Me is brought to you by Prodigy, a data annotation tool from Explosion AI. Prodigy is created by Ines Montani and her team at Explosion, and she's been doing machine learning and NLP for a long time. Ines is a friend and frequent guest on the show, and if you've listened to any of her episodes, you know that she knows her ML tools.
So what is Prodigy? It's a modern, scriptable annotation tool for machine learning developers made by the team behind the popular NLP library, spaCy. You can easily and visually annotate and develop data for named entity recognition, text classification, span categorization, computer vision, audio, video, and more, and put your model in the loop for even faster results.
After collecting data, you can quickly train and export a custom spaCy model or download annotations to use it with any other library or framework.
Prodigy is entirely scriptable, in Python of course, the language we all love, and it seamlessly integrates with your favorite libraries and tools.
Plus, the new alpha version they just released also introduces a built-in support for large language models such as OpenAI's GPT models, and new tools for dividing up your data between multiple annotators. TalkByThon listeners get a massive discount on a lifetime license.
You'll get 25% off using the discount code Talk Python.
But don't wait too long. This offer does expire.
Get Prodigy at Talk Python.fm/Prodigy and use our code Talk Python, all caps, to save 25% off a personal license.
This link is in your podcast player show notes.
Thank you to Explosion AI for sponsoring the show.
I think it really depends on what kind of data scientist you are.
if what you are is someone doing research, as you described before, you're like, is there a trend between the type of device that they use to buy their thing at our store and how much they're buying on the second, you know, how much are likely to come back? Like if they're using an iPhone, do they, do they tend to spend more than if they're using an Android? And is that a thing that we should consider or, you know, is there any like that kind of exploration, which you can judge whether or not you should make that exploration, but just put that aside for a minute. That kind of stuff, like once you know that answer, maybe you don't need to run that code again. Maybe you don't care. You just you just want to kind of discover if there is a trend. And there, maybe you need to know software engineering techniques, but should you be writing unit tests for that?
I'd say maybe not, honestly. On the other hand, if your job is to create a model that's going to go into production, that's going to run behind a flask or fast API endpoint, then you're kind of in the realm of continuously running for many people over a long time. And I think that really is a different situation.
I think this is where you actually move from data science to machine learning engineering.
This term has a lot of different definitions. For me, I base my definition of ML engineers on the two people that I've worked with, who were like true full stack kind of people who could go from research and prototyping to deployment.
And they were data scientists who really cared enough to actually learn how to do proper engineering, and they could actually deploy their own things.
But then this leads to another one of my very favorite topics, which is who is responsible for apps in production.
And here's the thing.
So I think as good as your data scientist is going to be, or your ML engineer, let's say an ML engineer, let's say that they can actually deploy their own code.
If they're then responsible for that code in production, that then eats up the time that they can be prototyping and researching new things for you. So the conclusion I've come to over time, and again, this is a matter of debate. This is just my opinion. Basically, I think if your company is above any sort of level of size or complexity in terms of the data products it has, I think you really do need dedicated data science and engineering teams. Because in the end, no matter how good your data scientist code is going to be.
It needs to be implemented by the person who's going to maintain it.
And maybe they're not the ones writing the code from scratch.
Maybe they can adapt the data scientist code if it's good enough.
But in the end, they need to be comfortable and familiar enough with that code to be like, yeah, if I get pinged at three in the morning, I'm OK knowing what to do with this code. Yeah, that's a good point.
Yeah. So I think it's just easier to scale these teams in parallel rather than trying to hire this like all in one person who can do everything.
they're impossible to hire.
Like I've only ever met two over the course of my career and quickly they become overwhelmed by having to maintain projects.
- Right, is that the best use of their time?
- Yeah, and like it's maybe it's not necessarily even if it's the best use of their time, it's more like then who's gonna do your research because now you've used up that resource on maintaining two or three projects.
- Right, absolutely.
Chris May's got an interesting question out here in the audience.
It's kind of turns us on a little bit.
It says, "Development teams tend to work better and they focus on writing and refactoring code to make it testable and understandable.
And we've talked a little bit about maybe stuff that data scientists shouldn't care about or whatever.
So are there ideas that are like good practices for data scientists and teams of them?
- This is actually a really great question.
So basically, it's an interesting thing with data scientists that unlike software developers, we often tend to work alone on projects or maybe in very, very small teams, like maybe two or three people.
And I think it's probably a hangover from the fact a lot of us at XAcademics, we're just used to having like, it's not great, but it's...
A whiteboard, an office in the corner and no one knows what you're doing.
Exactly. And no one cares. That paper that three people read, took me three years. So what I think has been neglected, you know, aside from learning software engineering best practices, is more fundamental things, which is like writing maintainable code. And I don't mean maintainable in the sense of it's a system that needs to be able to run regularly. It's more like this is a piece of code that I can come back to in six months and understand what I was doing. Because, you know, research projects can be shelved forever, but maybe they need to be revisited and, you know, built upon. So, this was actually a topic I got really interested when I first move to industry, like the idea of reproducibility with data science projects.
It's about the code, but it's also about things like dependency management, which is notoriously difficult in Python to get reproducible environments later.
And even the operating system.
If Linux has really dramatically changed over time, then maybe, maybe your, your dependency, your old dependency, you want to keep that one, but it won't run on the new operating system or there's a whole spectrum of challenges there.
Exactly. Exactly. And it's sort of something that can be solved with using poetry, which is a little bit more robust.
But even then, it's you've still got it like it runs on my machine effect where your machine will not be the same machine.
Increasingly, there's actually a move towards doing more sort of cloud-based stuff for data science, which solves a few of these problems.
And it also solves the additional problem where data scientists often need to do remote development for various reasons.
like you need access to GPUs in order to train models.
So, you know, obviously, if you have a server, you have a Docker container which has environment specifications, you can power up that exact same environment. And that actually helps with that reproducibility a lot.
And then another point which I think is really important for data scientists and can be neglected is literate programming.
So this is an idea from Donald Knuth.
And it's this idea that you should write your code in such a way that it's actually understandable later. With data science work, it's also that you really need to document a lot of the implicit kind of assumptions that you make or decisions that you make as part of the research project process. And this is one of the reasons, probably a good segue, why Jupyter is so important. Jupyter notebooks are designed to be research documents.
So this is why you have the markdown cells if you've seen a Jupyter notebook, because it's this idea that you really, really need to document along with the code, the decisions that you made. Like, why did you choose this sample? Why did you decide to create the inputs to your models the way that you did it? You need to document all this stuff. So, yeah, reproducibility is a super interesting topic. And I think it's, yeah, something that really needs to be thought about carefully, even if you're not collaborating with anyone else, because otherwise your piece of research is going to be worthless in three months, 'cause you're not gonna remember what you did.
- I think notebooks are quite interesting.
They go a long ways to solving that.
When used in the right way, you can just jam a bunch of non-understandable stuff in there and it's just, well, now it's not understandable, but it's in a webpage instead of in an editor.
But I think as in, you know, not just programmers, but tech in general, we're just bad at thinking about the long-term life cycle of information and compute.
For example, I got a new heat pump to replace the furnace at my house. The manual for it came on a CD drive and I'm like, I don't think I have a CD. Where did I put that? I would go dig through a closet full of electronics. I'm not sure I can read that, right? And CDs seem so ubiquitous for so long, right? And just simple little mismatches like that just get worse over time. It's going to be tough to keep some of this older research and reproducibility around. >> Yeah, like it's super interesting that There are packages I used to use, you know, back when I first started in natural language processing.
Some of them haven't been updated from Python 2.
So I can't use them anymore.
Because they were just some, probably like a PhD project, and no one really had the time or energy to maintain it after that person graduated.
And the person graduated, got a job and doesn't really care that much anymore, potentially.
Exactly.
Not enough to keep it going.
Yeah, it's not even necessarily their fault.
It's just life.
Yeah, yeah.
Let's talk about some of the libraries and tools.
You mentioned Jupyter.
I think Jupiter is one of the absolute cornerstones, right?
So, Jupiter or Jupiter Lab? What are your thoughts here?
It's funny, actually, for years I was just working in Jupiter, playing Jupiter on my computer.
Maybe give people a quick summary of the difference, just so, who don't know.
Very good idea. So basically, Jupiter is, I suppose you could call it an editor.
It's basically an interactive document which you run against a Python kernel, or you can run it against different language kernels. There are, there are Julia, there are Kotlin notebooks. Should I give my little advertisement for JetBrains? Basically, what you can do is you can run code in cell blocks. Then you can also create markdown cells in between them. And this allows you to basically have markdown chunks and then cell chunks.
JupyterLab is hosted remotely. So you have basically a bunch of other functionality built in so you can open terminals, you can create scripts, things like that. But basically, It's like a little Jupyter ecosystem, which is designed to be remotely hosted, and it can be accessed simultaneously by several people.
So I would say Jupyter is good if you are just starting out and you're dealing with small data sets.
Maybe you're even retrieving things from databases, but you're not saving anything too heavy locally.
You're not using a huge amount of memory, like maybe unless you got one of those new M2 Macs and server in your office.
So go for it.
Yeah, JupyterLab, I think is good if basically you need to access different types of machines.
So maybe you need to be able to access GPU machines easily. You kind of want that remote first experience where you don't have to then connect to a remote machine. And I have found JupyterLab helpful in the past for sharing. But the thing you can't do with JupyterLab is real-time collaboration. And that's a bit of a pain in the butt. Obviously, since I started at JetBrains, I kind of, you know, like I'm using our tools and I like them a lot.
Obviously I wouldn't advocate them.
Yeah, I was going to ask, is this PyCharm, Dataspell?
When you actually do that, are you using some of those type of tools?
I am. So I won't turn this into too much of an advertiser for our tools, because it's not really the point of me being here.
But we've kind of tried, or my teams have tried to solve some of these problems that you might have with just using plain Jupyter notebooks, or even working with JupyterLab, maybe a bit more, like, robustly.
So we have actually three data science projects, products.
We have PyCharm and Dataspell, which you've mentioned.
They're desktop IDEs with the ability to connect to remote machines, but they're not really collaborative, but they do give you like really like nice experience with using Jupyter debugging and co-completion and all those sort of things.
We have another one, which is Datalore.
And this falls into those managed notebooks that I was talking about, it's cloud hosted.
And the nice thing about Datalore actually you can do real-time collaboration. So it sort of helps overcome...
- Comp style, sort of.
- Yes, it's the same technology, actually. So...
- Okay.
- Yeah. So it's kind of a very interesting thing because there will be times where, you know, maybe you're not working on a project with a data scientist, but you need them to have a look at your work. And when I was working with JupyterLab, what we would do is we would clone the notebook to our own folder, and then we were in the same environment, so it was okay. And you would rerun the whole thing again. And sometimes it would be pretty time consuming.
Datalore is an alternative to that. It may or may not be kind of your style. But it's pretty cool because you can actually just invite someone to the same notebook instance that you're in, and you're basically hosting them. And they have access to everything that you've already run.
So it's like true kind of real time.
Yeah, that's nice. Because sometimes a cell has to run for 30 minutes, but then it has this nice little answer and you can work with that afterwards, right?
- Exactly, or you want a model to be available and maybe you haven't saved it or something.
Like this is just a way around some of these friction points.
- I want to circle back just really quickly for a testimony, I guess, out in the audience.
Michael says, "I started teaching basic Git, "Docker and Python packaging "to bioinformatics students at UCLA "and it's made a huge difference in the handoff." And I think for actual projects, you know, I just think, as we were talking about what should people learn at data science and what they shouldn't, A little bit of the fluency with some of these tools is really helpful.
I absolutely agree.
I know it can be really overwhelming, especially Git initially for students.
Git is overwhelming at first.
Yeah. I would say because I tend to work on things by myself.
Yeah. This falls into the reproducibility and stuff that I was talking about earlier.
It's super, super important.
Once you get comfortable with just basic use of these tools, you can get really far.
Okay. Back to some of the tools, Jupyter, JupyterLab.
What about JupyterLite?
Have you, have you played with JupyterLite?
Any?
Only a teeny tiny bit because of this workshop that I'm going to be helping out with at EuroPython.
So they're going to be running the whole thing in, in JupyterLite, hopefully.
Couple of bugs to solve, but I think they're overcomable, but yeah, it's a really interesting alternative to Google Colab actually.
Yeah.
JupyterLite, take Pyodied, which is CPython running a WebAssembly, and then build a bunch of the data science libraries like Matplotlib and stuff in WebAssembly.
And then the benefit is you don't need a complex server to handle the compute and run arbitrary Python code, which is a little sketchy.
You just run it on the front end in WebAssembly, which is pretty cool.
I interviewed the folks at PySport a little while ago.
And it's just the ability to just take code and run all these different pieces on your front end without worrying about a server, I think is super cool.
If I get that right or not, but anyway, just I think running it on top of people using it, on top of the browsers, like you do JavaScript, is it's an interesting thing to throw into the mix for notebooks.
- Actually, a lot of these projects coming out using Pyodide are really interesting.
Obviously, PyScript is the big one from last year.
- Yeah, I think PyScript actually has really lots of interesting possibilities beyond just the data science side, right?
Whereas Pyodide is a little more focused on just, I think, really providing the data science tools on the client side.
We'll see where PyScript goes.
If they can make an equivalent of Vue.js or something like that, where people can start building legitimate front-end interactive web apps, like Airbnb or Google Maps or something, but with Python, that's gonna unlock something that has been locked away for a really long time.
With PioDyte, you know, that's like a nine or 10 meg download.
That's too much for the front end, just for like a public facing site generally at the start of time, but they're moving it to MicroPython as an option.
And that's a couple hundred K, which is like these other front end frameworks.
So it's very exciting.
I think that's going to be, that's definitely the most exciting thing in that area.
But all right, back to data science.
Let's see where you want to go next.
You want to talk pandas maybe?
- Yeah, let's jump into pandas, which is the other, the other biggie when you're talking about data science.
So what pandas is really important for is, it's basically the entry point of you working with your data. So it's a library, which basically allows you to work with data frames, data frames, basically tables. And from there, you can do data manipulation, you can explore your data and visualize it. And it also is an entry point to passing your data into models. Sometimes it'll need additional transformations, but say scikit-learn, which we can talk about in a sec, you can basically pass Panda's data frames directly into scikit-learn models.
Panda's also, because of its popularity, has kind of opened up this easy access to grid computing and other types of processing database stuff that you don't really need to learn those tools, but you get to take advantage of. And so two things that come to mind for me are Dask.
Yes.
It's kind of like a Panda's code, but instead, you can say actually run this across this cluster of machines or larger than memory or stuff on my personal computer or even just take advantage of all 10 cores on my M2 instead of the one.
Yes.
Have you done anything with Dask? Are you a fan of it?
I was kind of there when Dask was new.
And let's just say they find out a lot of the bugs.
Yeah, yeah.
So what ended up happening was I ended up learning PySpark instead.
So I went down a different kind of route.
But I think, you know, they solve very similar problems.
It's just Dask is much more similar to pandas.
And so you don't really need to deal with learning.
It's similar, but it's a new API.
Yeah. Another one that I was thinking of, I just had these guys on the show, sort of, is Ponder.
Oh, I have not heard of this.
So Ponder, they were at startup row at PyCon.
And they basically build on top of Moden, which is important, moden.pandas as PD.
And what it does is it, instead of pulling all the data back and executing the commands on your machine in memory, which maybe that data transfer is huge, it actually runs it inside of Postgres and other data, and I think PySpark as well.
Like it translates all these pandas commands to SQL commands to run inside the database where the data is, which is also a pretty interesting thing.
That is amazing.
So yeah, it's just interpreting the code in a completely different way.
You can do like query planning and optimize the code.
Yeah, exactly.
- I think they said that df.describe is like 300 lines of SQL.
It's really, really tough.
But once this thing writes it, then it's good to go.
And I think the reason I bring this up is like, you don't have to write that code.
You just have to know Pandas.
And then all of a sudden, there's these libraries that'll do either grid computing or really complex SQL queries that you don't care about.
- Yes. - You don't care to write or so on.
So I think it's, Pandas is interesting on its own, but it's almost like a gateway to the broader data science community.
- Agreed, agreed.
And it's such a de facto, I think, for data analysis now or data manipulation transformation.
Yeah, like I don't see it going away anytime soon.
And actually, Pandas 2.0 just came out.
And instead of being, yeah, instead of, Pandas is NumPy under the hood, which is fast, but it's not really equipped to deal with certain kinds of structures like strings, because, you know, it's not really what NumPy is about.
and also missing values, the way that it handles it is pretty janky.
So yeah, it's been rewritten with PyArrow under the hood.
>> Right.
>> Yeah. Apparently, the performance is so much better.
Something I need to sit down and actually try.
It's been out for like a month and I'm feeling a bit bad, but yeah.
>> Yeah, that's cool. It probably has support for some of the serialization formats to back up our term like, I said, Parquet and some of those types of things.
I think that comes straight out of PyArrow.
>> Yeah.
Excellent. So that kind of brings me to a trade-off I wanted to talk to you about before we get off of pandas.
Although it sounds like pandas 2.0, it makes this less important.
But you know, another sort of competitor that came out is Polars, which is a data framing library for Python written in Rust.
Many of the things are written in Rust these days when they care about performance.
It's like a big trend. It's the new C extensions of Python.
But this one is supposed to also be way faster than pandas 1.
and I think it's also based on PyArrow amongst other things.
The details are not super important.
More what I wanted to ask you is like, well, here's another way.
This is a totally different API.
It doesn't try to be compatible, so you got to learn it.
The question is, as a data scientist, as a data science team leader, how should you think about, do we keep chasing the shiny new thing, or do we stick with stuff that one, people know like pandas, but two, also extends into this broader space as a gateway, as we described, like, what are your thoughts here?
This is a super interesting question. So data scientists in some ways, have the luxury of being able to maybe use newer packages faster, because we build these small kind of atomic projects that we can just update to the next library that we feel like using in the next project. And maybe we're the only ones who ever look at that code. So it's cool. The The problem is though, of course, is if someone else needs to look at your code, they are gonna need to be able to read it, which is not maybe the biggest problem.
The biggest problem of course, is any new library, you have less documentation and you have less entries on stack overflows.
So I would say you need to make a trade-off between the time you're not only gonna spend learning it, but also debugging it, 'cause it's gonna be slower, but your ChatGPT doesn't know much about polars.
basically you're essentially going to need to trade that off against, are you gonna see a benefit from that?
So do you actually have problems with processing your data fast enough?
If you're working on small data sets, probably not.
If you're not, then maybe try something pandas or pandas adjacent.
- Yeah, that sort of community support side is important.
And I'm pretty sure there are a lot of data scientists out there who are the data, the one data scientist at their organization.
And so it's not like, - Oh, we'll go ask the other expert down the hall because if it's not you, there's no answer, right?
- Exactly.
I do think though, like, it's good to be curious.
It's good to try out new things as well.
And again, part of being a data scientist is you can experiment a bit more.
So--
- You know, 2017, 18, sort of the peak Python two versus three tension, I guess, maybe one year before then.
I noticed that the data scientists were like, I don't know what y'all are arguing about.
we're done with this.
What we're arguing about is, when can we take the Python 2 code out to absolutely 100% drop support for it, not when are we moving over?
Whereas people running that Django site that's been around for eight years, that's still on Python 2, they're starting to get nervous 'cause they don't wanna rewrite it 'cause it works, but they know they're gonna have to.
And I feel like, we talked about the legacy code as sort of the success story that is dreaded of software on the computer science side.
because that is less of a thing in data science, it's easier to go, well, this next project that we're starting in a couple of months, we can start with newer tools.
- Yeah, and I actually remember the point where I decided, okay, this is the last project I'm doing into because the thing that was keeping me into was actually one of those libraries that I mentioned, which built by a university.
And I was like, you know what?
I'm just gonna go find some alternative tool.
I think at that time, Spacey, which is a very well-known NLP library, actually, based here in Berlin, the company.
- Yeah, exactly, basically a neighbor of yours.
- That's right.
But I think Spacey was really getting off its feet in that time.
So I was like, you know what, I'm just gonna switch over to this new library and try that.
And it's excellent.
So I didn't look back.
- Yeah, Spacey's cool.
Enos Montani is doing really great work and everyone over at Explosion AI.
And that's the thing, sometimes it seems like a hassle, right, but if it forces you out of your comfort zone to pick stuff that's being actively developed, maybe it's worth it, right?
- Exactly.
- All right, we're getting short on time.
So you want to give us a lightning round and the other important libraries you think data scientists should pay attention to?
- Yeah, so let's just quickly go through the visualization side of things.
So visualization is massive.
So matplotlib is really the biggie and it's what a lot of libraries are actually built on top of in Python.
So the syntax is not that friendly.
So there's a lot of alternatives.
So Seaborn is a very popular one.
we actually have an internal one called Let's Plot, which is a port of ggplot2, and there's another one called plot9, and I think there actually may even be one called ggplot.
Plotly--
- Some of the fancy new ones that people hear about, they're actually internally just controlling Matplotlib in a cleaner API, right?
- Pretty much, and let me tell you, Matplotlib needs a clean API, it's a bit, let's say archaic.
- Although, give it some props for its XKCD graph style.
I mean-- - Yes, yes.
- That is pretty cool that you can get it to do that.
- I actually have done, I've done XKCD graphs in Python as well.
It's a goal that you aim for to do like elite visualizations.
- It's fun and XKCD is amazing in a lot of ways.
However, I think it also can serve an important role when you're presenting to like leaders of an organization, non-technical people, 'cause if they look and see a beautiful, pristine production already, sort of like, we're done.
No, no, no, this is the prototype. No, we're done. Look, you look, you already got it.
But if it comes out and sort of cartoony, kind of like wireframing for UI design, you're like, oh, there are no expectations. It's done. It's XKCD. We're going to get you the real graphs later, right?
Yeah, yeah.
There may be some value there.
Like a psychological effect where you make it look like a hand-drawn prototype.
Exactly. It looks just hand-drawn. It's barely done.
That's right.
It's really just theme equals.
It didn't take me two days.
[Laughter]
Scikit-learn, you mentioned that before.
Yes. So there are a whole bunch of libraries for doing machine learning. Scikit-learn is kind of your all-in-one for classic machine learning. But then, you know, you have this whole other branch of data science, which is around neural nets or deep learning. So you have Keras, TensorFlow, you have PyTorch, and then you have a package for working with a lot of like these generative AI models or large language models called Transformers from a company called Hugging Face. So, all of these are actually super accessible. I wouldn't say TensorFlow and PyTorch can be tricky, but Keras is like a friendly front end for them. Actually, if anyone is interested in getting into this side of things, there's a book called Deep Learning in Python by a AI researcher at Google called called Francois Chollet.
It is actually, I think, the most popular book ever on Manning.
So it's an amazing book.
I can only recommend it.
And it's very gentle for beginners who have no background in the area.
- Okay, yeah, cool.
I'll put that in the show notes.
- Awesome. - Yeah.
All right, well, there are many other things we can talk about.
Maybe just let's close this out with a quick shout out to your PyCon talk.
Eventually, someday, I'm sure that the talks for PyCon will be on YouTube.
They were last year, but I looked back and I was so excited near the end of the conference, I'm like, "Look, the talks are up." And I was talking to someone like, "Look, here's your talk." They're like, "No, that's my talk from last year." I'm like, "Oh." - Aw.
- Yeah, so it was maybe three or four months delayed till it actually came out.
So maybe this midsummer, the video of Virginia Talk will be out, but maybe just give people a quick elevator pitch of your talk here.
- Yeah, so I decided to give this talk because I kind of had to learn things the hard way in terms of performance with Python.
So basically I used to do everything with loops and then I had to start working with larger amounts of data and it just doesn't scale.
So over time, as I got better with Python, I learned more about NumPy, which is another important data science library.
And it basically allows you to do what's called vectorized operations.
So in this talk, I basically talk about like the math behind why vectorized operations work.
You don't need any math background to understand it.
It's very gentle.
And then just show like why some of these operations work in NumPy and how you can implement it yourself to get like really like massive gains in performance speed.
- Yeah, that's incredible.
Move a lot of that stuff down into like a C or a Rust layer and just let it do its magic instead of looping in Python.
Yeah. - Exactly.
- Yeah, very cool.
So I don't know when, but eventually this will be out as a video people can check out from me.
Now they know to go look for it.
- Yeah, I think the port team is still recovering.
So much work.
- I know.
All right, well, Jody, it's been great to have you on the show.
Before you get out of here, final two questions.
If you're gonna write some Python code, what editor are you using these days?
- So I'm actually using all three that I talked about.
I use PyCharm if I need to do something like a bit more on the engineering side, which is not that often for me.
Data Spell, if I'm doing sort of very local development and doing more of the research side, and then if I need some GPUs, I'm using Datalore.
So a bit boring, but using all of our tools, and I really like them.
- Yeah, they are good.
All right, and then notable PyPI package, something you wanna give a shout out to, or if you prefer a conda package, there's a lot of intersection there.
- I think my favorite package at the moment is Transformers.
It is amazing.
And the documentation that Hugging Face have put together is so good.
And just the work they're doing in open data science is so, so important.
So like big props to Hugging Face.
Like we should really support the work that they're doing.
- Excellent.
All right, well, thanks for being on the show and sharing your experience.
- Thank you so much for having me.
I had an absolute blast.
- Yeah, same.
Bye. - Bye.
- This has been another episode of Talk Python to Me.
Thank you to our sponsors.
Be sure to check out what they're offering.
It really helps support the show.
The folks over at JetBrains encourage you to get work done with PyCharm.
PyCharm Professional understands complex projects across multiple languages and technologies, so you can stay productive while you're writing Python code and other code like HTML or SQL.
Download your free trial at talkpython.fm/donewithpycharm.
Spend better time with your data and build better ML-based applications.
Use Prodigy from Explosion AI, a radically efficient data annotation tool.
Get it at talkpython.fm/prodigy and use our code TALKPYTHON all caps to save 25% off a personal license.
Want to level up your Python?
We have one of the largest catalogs of Python video courses over at Talk Python.
Our content ranges from true beginners to deeply advanced topics like memory and async.
And best of all, there's not a subscription in sight.
Check it out for yourself at training.talkpython.fm.
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.
We should be right at the top.
You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.
We're live streaming most of our recordings these days.
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.
This is your host, Michael Kennedy.
Thanks so much for listening.
I really appreciate it. Now, get out there and write some Python code.
[MUSIC]
