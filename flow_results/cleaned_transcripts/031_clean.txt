Alexandre, welcome to the show.
Thank you, hi.
Hi. I'm really excited to talk about machine learning and Scikit- learn with you today. It's something I know almost nothing about, so it is going to be a great chance for me to learn along with everyone else who is listening.
So hopefully I will be able to give answers.
Yeah, I'm sure that you will. All right, so we are going to talk all about machine learning, but before we get there, let's hear your story, how did you get into programming in Python?
Well, I've done a lot of scientific computing and scientific programming over the last maybe 10 to 15 years, having started my undergrad in computer science doing a lot of signal and image processing. Well, like these types of people have done a lot of Matlab, in my previous life and-
Yes, I've done a lot of Matlab too, I know about the dot in files.
And, I did a phd in computer science applied to brain imaging and I switched to a different team where I got surrounded by people working with Python, and basically I got into it, and I switched like in one week, Matlab has gone from my life. And, it has been maybe five years now. And, that's kind of the historical part.
Do you miss Matlab?
Not really.
Me neither. And there are some cool things about it, but-
Yeah, so I have students insisting to work with me in Matlab, I have to still do stuff in Matlab for supervision, but not really when I have the choice.
Yeah, we have the choice, of course. I think one of the things that is really a draw back about specialized systems like Matlab, is it's very hard to build production finish products. You can do research, you can learn, you can write papers, you can even test algorithms, but if you want to get something that is running on data centers on its own, probably Matlab is- you could make your work but it's not generally the right choice.
Definitely, yeah.
Yeah, I think that explains a lot of the growth of Python, in this whole data science scientific computer world, along with great tool kits like Scikit-learn, right?
Yes, that is definitely the way Sciki-learn is now used, just the fact that in Python stack allows you to make this 4:31 type of code is a clear win for everyone.
So, before we get into the details of Scikit-learn and how you work with it and all the features it has, let's just in a really broad way talk about machine learning- like, what is machine learning?
I would say the simple example of machine learning is trying to predict something from previous data. So what people would call supervised learning, and plenty of examples of this in everyday life like your mail bug that predicts for you if your email is a spam or ham. And that basically is a system that learns from previous data how to make it inform choice and give you a prediction. And as basically the most simple way of seeing machine learning and basically you see machine learning problems framed this way in all context, from industry to academic science and I mean, there are many examples. And basically the in terms of other types of classes of problems that you see in machine learning is not really these prediction problems we try to make sense from raw data where you don't have labels like spam or ham, but you just have data and you want to figure out what is the structure, what types of input or insight can you get from it and that's I would say the other big class of problem that machine learning addresses.
Yeah, so there is that general classification. I guess with the first category you were talking about like spam filters and other things that maybe fall into that realm, like credit card fraud, maybe trading stocks, these kind of binary do it/ don't do it based on examples, that's something that is called structured learning, what's the-
The common name is supervised learning.
Supervised learning, that's right.
Yeah. So basically you have pairs of training observation that are 6:39 their corresponding labels. So text and the label would be spam or ham. Or you can also see, this is basically binary classification, the other types of machinery problems you have, for example regression- you want to predict the price of the house, and you know the number of square feet, and you know the number of rooms, you know what is exactly the location and so you have a bunch of variables that describe your house or apartment and from this you want to predict the price. And that is another example where now since the price is a continuous variable, it's not binary, this is what people call regression, and this is another class of supervised learning problem.
Right, so you might know through the real estate data all the houses in the neighborhood that are sold int he last two years, the ones that are sold last month, are there variables, dimensions if you will like number of bathrooms, number of bedrooms, square feet and you could- square meters and you can feed it into the system to train it and then you can say, "Well now I have a house with two bathrooms and three bedrooms and right here- what's it worth?" Right?
Exactly, that is basically a typical example and also a typical data set that we use in Scikit- learn. That basically illustrates the concept of regression with the similar problem.
Right, there is- we'll talk more about it but there is a Scikit- learn comes with some pre- built data sets and one of them is the Boston house market, right?
Exactly, that's the one.
Yeah. How much data do you have to give it, like suppose I want to try to estimate the value of my house, which at least in the United States we have the service called Zillow  so, they are doing way more, I am sure they are running something like this actually. But suppose I want it to take upon myself to like grab the real estate data and try to estimate the value of my home. How many houses would I have to give it before it would start to be reasonable?
Well, that's a tough question, and I guess there is no simple answer. I mean, you have this, that you can see under cheat sheet of Scikit- learn that says if it is less than 50 observations then go get more data. But I guess there's also a simplified answer, it depends on the difficulty of the test, so at the end of the day often for these types of problem you want to know something and this can be easy or hard, you cannot really know before trying, and typically regression it would say ok, if I predict the 10% plus or minus that's maybe good enough for my application and maybe you need less data. If you want to be super accurate you need more data, but the question of how much is it's really hard to answer without really trying and using actual data.
Yeah. I can imagine. And it probably also depends on the variability of the data, the accuracy of the data, how many variables you are trying to give it, so if you just try to base it on square footage or square meters of your house, that one variable maybe it's easier to predict than, you know, 20 components that describe your house, right?
So the thing is, the more variables you have, the more you can hope to get. Now it's not a simple as this, because if variables are not informative, than they are basically not adding knowledge to your problem. So want as many variables to describe your data, in order to like capture the weak signals, but sometimes just the variables are not relevant or predictive. And so you want to remove them from the prediction problem.
Ok. That makes sense. So, I was looking into what are some of the novel uses of machine learning. I sort of have some things to ask you about, and just see what is out there. What are ones that come to mind for you and then I'll give you some that I found on my list.
Maybe I biased because I'm really into using machine learning for scientific data and academic problems, I guess for the things that are really academic breakthrough that are reaching everybody is really, really computer vision and NLP these days, and probably also speech. So these types of system that try to predict something from speech signals of from images like describing you what's the contents, what types of objects you can find and for NLP you have like machine translation-
We did a show on open CV and the whole Python angle there, there is a lot of really cool stuff on medical imaging going on there, does that have to do with Scikit- learn as well?
Well, you have people doing medical imaging in Scikit-learn, basically extracting features from MR images- magnetic resonance images, or CT scanners, also like EEG brain signals, and they are using Scikit- learn as the prediction to all. Deriving features from the raw data, and that reaches of course clinical application to some context.
Maybe automatic systems that say, hey, this looks like it could be cancer, or it could be some kind of problem bringing the attention of an expert who could actually look it and say yes/ no. Something like this?
Yeah exactly. It's like helping diagnoses like trying to help to isolate something that looks weird or suspicious in the data, to get like the time of this physicist and equation on this particular part of the data to see what is going on and if the patient is suffering from something.
Right, that's really cool, I mean maybe you can take previous biopsies and invasive things that have happened to other people, and there are pictures and the outcomes and say, look, you have basically the same features and we did this test and the machine believes that you actually don't have the problem, so probably no worry about it. Or something like that, right?
Yeah, I mean on this line of code there was recently a competition using retina pictures so like people suffering from diabetes usually have problems with retinas and so you can take pictures of retinas from hundreds of people and see if you can build a system that predicts something about the patient and the state of the disease from these images and this is typically done by pulling data from multiple people.
That's really cool. I've heard this Kaggle competition or challenges before in various places, what is that?
So it's basically a website that allows you to organize these types of supervised learning problems, where a company or like a structure, NGO whatever is having data and is trying to build a predictive system and they ask Kaggle to set this up which basically means for Kaggle putting the training online and giving this to data scientists and they basically spend time building a predictive system that is evaluated on new data on which to get a score. And that allows to see how the system works on new data, and to rank basically data scientists that applying this system. It's got an open innovation approach in data science.
That's really cool. So that's just kaggle.com?
Yes. Exactly.
Yeah, very nice. Some of the other ones that I sort of ran across while I was looking around that were pretty cool was, one is some guys at Cornell university built machine learning algorithms to listen for the sound of whales in the ocean and use them in real time to help ships to avoid running into whales. That's pretty awesome, right?
Yeah, there was a Kaggle competition on these whales sounds maybe a couple of years ago, and it was, basically I mean that many days the scientists have experienced like listening to whales. So it's kind of everybody doesn't really know what types of data, and I remember this presentation from the winner basically saying how to win a kind of competition without knowing anything about the data, it was kind of the provocative talk but showing how you can basically build a predictive system by just looking at the data and try to make sense of it without really being an expert in the field.
Yeah, that's probably a really valuable skill as a data scientist to have, right, you can be an expert, but not in everything. Some other ones that were interesting was, IBM was working on something to look at the handwritten notes of physicians. And then it would predict how likely the person that those notes were about would have a heart attack.
Yeah, in the clinical world it's true that the lot of the information is actually low text, like manual, like just written notes, but also raw text on the system. For machine learning that's particularly difficult problem because what we call unstructured data, so you need typically for Scikit- learn to work on this type of data you need to something extra, to basically come up with the structure, come up with the features that allow you to predict something.
Sure, and so both of those two examples that I've brought up have really interesting data origin problems. So if I gave you an mp3 of a whale, or audio stream of a whale, how do you turn that into numbers that go into the machine even to train it, and it is similar with handwriting, how do you- you've got to do handwriting recognition, you've got to then do sort of understanding what the handwriting means and there is a lot of levels. How do you take this data, like and actually get it into something like Scikit- learn?
So, Scikit- learning expects that every observation, we also call it a sample, or a data point, is basically described by a vector. Like a vector of values. So if you take the sound of the whale, you can say, ok just the sound in the mp3 is just a set of floating point values like every time samples really time them in signals that you get for a few seconds of data. It's probably not the best way to get a good predictive system, you want to do some feature transformation, change the input to get something that brings feature that are more powerful for Scikit- learn and the learning system. And you would typically do this with time frequency transform, things like spectrograms, trying to extract features that are really for example 17:36 to some aspects of the data like frequencies or time shifts. So they probably a bit of pre-processing to do on this raw signals and then once you have your vector, you can use the Scikit- learn machinery to build your predictive system.
How much of that pre-processing is in the toolset?
So it depends for what types of data, typically for signals there is nothing really specific in Scikit learn you would probably use SciPy signal, or any type of signal processing Python code that you find online. I would say for other types of data like text, there is in Scikit learn this thing called "feature extraction module" and you have in the feature extraction module you have something for text which is probably the biggest part of the feature extraction is really text processing. You have some stuff also for images but it's quite limited
So, we should probably introduce what Scikit- learn is and get into the details of that, but I have one more sort of example to let people know about that I think is pretty cool. On show 16, I talked to Roy Rapoport from Netflix and Netflix has a tremendously large cloud computing infrastructure to power all of their, movies system and everything behind the scenes there. And they have so many virtual machine instances and services running on them and then different types of devices accessing services on those machines that they said it's almost impossible to determine if there is some edge case where there is a problem, manually and so they actually set up machine learning to monitor their infrastructure and then tell them if there is some kind of problem in real time.
Yeah.
So I think that that is really a cool use of it as well.
Yeah, that's a very cool thing to do and actually many industries and many companies are looking for these types of systems that they like anomaly detection, or failure prediction. And that's like, it's getting a big use case for machine learning indeed.
The Netflix guys were actually using Scikit learn not some other machine learning system, so let's get to the details of that, what's Scikit learn, where did it come from?
So, Scikit- learn is probably the biggest machine learning that you can find in the Python world. So, it dates back from almost ten years ago when David Cournapeau was doing a Google summer of code to kickstart the Scikit- learn project. And then for a few years a friend Matthieu Brucher took on the project, but it was kind of a one guy project for many years and in 2010, with colleagues at INRIA and friends we decided to basically try to like start from this state of Scikit learning and make it bigger and really try to build a community around this. So these people are Gael Varoquaux and Fabian Pedregosa, and also somebody you may have heard of in the machine learning world Olivier Grisel. And so that was pretty much 2010, so five years ago and it basically took on pretty quickly, after I would say a year of Scikit- learn we had like more than ten core developers way beyond the initial lab where it started.
That's really excellent, yeah, I mean it's definitely, absolutely main stream project that people are using in production these days, so congratulations to everyone on that, that's great.
Thank you.
Yeah. And so the name Scikit- learn comes from the fact that it's basically an extension to the SciPy pieces, right? So SciPy is like NumPy for numerical processing SciPy for scientific stuff, Matplotlib, IPython, SimPy for simbolic math and Pandas. And then, there is these extensions.
Yes. So, basically the kind of the vision is that you cannot put everything in SciPy. SciPy is already a big project and the idea of the Scikit were to build the extensions around SciPy hat are more domain specific. Also, it's kind of easier to contribute to a smaller project, so it's basically- the barrier of entry for newcomers is much lower when you contribute to a Scikit then to a SciPy which is a fairly big project now.
Yeah, and there is so much support for the whole SciPy system right, so it's much better to just build on that and try to like duplicate and say NumPy or whatever.
Exactly. I mean, there is a lot of efforts to see what could be NumPy to point o and what could be the future of it and how to extend it. I mean, a lot of people are thinking of what's next, because I mean NumPy is almost ten years old. Probably more than ten years old now and yeah, people are trying to see also how it can evolve.
Sure. That makes a lot of sense. So speaking of evolving and going forward, what are the plans with Scikit- learn, where is it going?
So, I was saying in terms of features, I mean Scikit- learn is really in consolidation stage. Scikit- learn is five years old, the API is pretty much settled, there is a few things here and there that basically we have to deal now, that basically due to early decisions in terms of API that needs to be fixed and I guess the big objective is to basically do Scikit- learn to 1.0, like the first fully stable in releasing terms of API because that is something that we've been talking about between the core developers for more than two years now, coming with this 1.0 version that stabilizes every part of the API.
Right. When final major cleanup if you can and then stabilize yeah?
Exactly. And in terms of new features, there are I mean you always have a lot of cool stuff that are around and you see the number of pull requests that are coming on top of Scikit- learn, it's pretty crazy and I would say huge maintainers effort in reviewing effort The features are coming in slowly now in Scikit- learn much more slowly than it used to be but I guess it's normal for a project that is getting big.
Yeah, it's definitely getting big, it has 7600 stars and 4500 forks on GitHub, so that's pretty awesome. 457 contributors, cool.
Yeah. I would say for every release we get to, we try to release every six months and for every release we get a big number of contributors.
So, maybe we could do like a survey of the modules of Scikit- learn just the important ones that come to mind. What are the moving parts in there?
So maybe something I know that both represent the part of the module that I 25:50 the most which is the linear model. And recently, the efforts on the linear models were to scale it up. Basically try to learn this linear models in out of core fashion to be able to scale to a data that do not fit in ram, and that's part of the I would say part of the plan for this linear model module in Scikit- learn.
That's cool, so what kind of problems do you solve with that?
The types of problem where you have like humongous number of samples and potentially big number of features, so there are not so many applications where you get that many number of samples, but that's typically text or lock files. These types of industry problem where you collect a lot of samples on a regular basis. You have there is examples also if you monitor an additional system like if you want to do what we discussed before about like predicted maintenance, that's probably use case where this can be useful. The other like module that also attracts a lot of effort these days is the assemble module especially a tree module, so for models like Random Forest so great in boosting which are very popular models that have been helping people to win Kaggle competitions for the last few years.
Yeah, I've heard a lot about these force and so on, can you talk a little bit about what that is?
So a Random Forest basically is a set of decision trees that you pull together to get a prediction that is more accurate. More accurate because it has less variance in technical terms. The way it works is you try to basically build decision trees from subset of data, subset of samples, subset of features in a clever way, then you pull all these trees in one big predictive model and for example if you do minor classification and you train a thousand trees, you ask for a new observation to the 1000 trees what's the label, is it positive or negative? And then you basically count the number of trees that are saying positive and if you have more trees saying positive, than you predict positive. That's kind of the basic idea of random forest and it turns out to be super powerful.
That's really cool. It seems to me like it would bring in kind of different perspective or taking different components or parts of the problem into a count so, you know some of the trees look at some features and maybe the other trees look at other features and then they can combine in some important way.
Exactly.
Yeah. Another one that I see coming up is the SVM module. What does that one do?
So, SVM is a very popular machine learning approach that was basically very big in the 1990s and ten years ago and still get some attraction. And basically the idea of Support Vector Machine which is the- SVM is the acronym for, is to be able to use kernels on the data and basically solve linear problems in an abstract space where you project your 29:09 . Let me try to give an example. If you take a graph, or if you take a text or if you take a string, that's not naturally something that could be represented by a vector. And when you do a SVM you have a tool which is a kernel that allows you to compare these observations like in kernel between strings, a kernel between graphs and one you define this kernel , and this kernel needs to satisfy some properties that I'm going to skip, then you can use these SVM to do a classification but also regression. And this is what you have in the end, you have module of Scikit-learn which is basically a very clever and efficient binding of the underlying library which is called SVM.
Ok. excellent. And is that used more in the unsupervised world?
It's completely supervised, when you do SVM is classificational regression that's supervised there is one use case of SVM in an unsupervised setting which is what we call the one class SVM, so, you just have one class, which basically means that you don't have  labels, you just have data and you are trying to see what are the data that are the less like the others, that's more like an anomaly detection problem or we call it also novelty detection, or outlier detection.
Maybe we could talk a little bit about some of the algorithms. As a anon expert in sort of the data science machine learning field, I go in there and I see all these cool algorithms and graph, but I don't really know like what would I do with that? On the side it says, there is all these algorithms it supports. So for example, it supports dimensionality reduction, like what kind of problem is what bring that in for?
I guess it's hard to summarize, the hundreds and hundreds of pages that you have in Scikit- learn, in the documentation, I tried to give you a big picture without too much technical detail, to tell you when these algorithms are useful and what they are useful for. And what are the hypothesis and what kind of output you can hope to get. As one of the strengths of the Scikit- learn documentation by the way. And so, to answer your question, dimensionally reduction like the 101 way of doing it is the principal component analyses, where you are trying to extract subspace that captures the most variance in the data and that can be used to do visualization of the data in low dimension.
If you do a PCA in 2 or 3 dimensions then you can look at your observation as a scatter plot in 2D or 3D. That's basically visualization, but you can also use this to reduce the size of your data sets, maybe without losing too much predictive power so you take like biggest data set you run a PCA, and then you reduce the dimension and then suddenly you have a learning problem which is in smaller data because you basically reduced the number of features. That is kind of the standard approaches which are visualization or reducing of the data set to have more efficient learning in terms of computing time but also sometimes in prediction power.
Ok, that makes sense, that's really cool. So, like if went back to my house example, maybe I was feeding like the length of the driveway and the number of trees in the yard, and it might turn out that neither of those have any effect on house prices, so we could reduce it to a small problem by having this whole PCA look, those don't matter, throw that part out. It's really about the number of bathrooms and the square footage or something.
Well, yes and no, that is kind of the idea but in this example of prediction of houses, you want to reduce the dimension in an informed way, because the number of trees in the yard can be informative for something but maybe not to predict the price of the apartment or the price of the house. So when you do dimensional reduction in the context of supervised learning, that can be also feature selection, or basically selecting the predictive features which ultimately leads to reduce data set because you remove features but that would be in the supervised context, when you do PCA you are really in unsupervised way, you don't know what are the labels you just want to figure out what's the variance in the data coming from, on which access and which direction should I look to see the structure.
Another thing that is in there are ensemble methods for predicting multiple supervised models. What's the story there, that sounds cool?
So, Random forest is an example of ensemble methods. When you have an ensemble is basically saying that you are basically taking a lot of classifiers, a lot of regressors, and you combine them in a bag of models or an ensemble of models. And then you make them collaborate in order to build a better prediction. And Random Forest is basically an ensemble of trees. But you can also do an ensemble of neural networks, you can do an ensemble of whatever model you want to pull and that turns out to be in practice of in a very efficient approach.
Yeah, like we are saying, the more perspectives. Different models, that seems like it's a really good idea. So you mentioned neural networks. So Scikit- learn has support for neural networks as well?
Well, you have a multi layer perceptor in which is like the basic neural network. I mean, these days in neural network people talk about deep learning.
I've heard about it, what's deep learning?
So deep learning basically, neural network 2.0. Where you take neural networks and you stack more layers. So, kind of the story there is that for many years people were kind of stuck with networks of 2 or 3 layers. So not very deep. And part of the issue is that it was really hard to train something that would add more layers. In terms of research, there was two things that came up, which is first, that we get access to more data which means that we can train bigger and more complex models, but also there were some breakthrough in learning these models, that allowed people to avoid overfitting. Trying to be able to learn these big models, because you have more clever ways to prevent overfitting and they basically led to deep learning these days.
Very interesting. Yeah, that's been one of the problems with neural networks, right, if you teach it too much than it only knows just the things you have taught it or something right?
Exactly. It basically learns by heart what you provide us, trading observations and thus being very bad when you provide new observations.
I want to talk a little bit about the data sets that come built in there. We have talked a little bit about the Boston one, and that's the Boston house prices for regression. One I hear coming up a lot is the one called Iris. Is that like your eye itself?
So Iris is the data set that we use illustrate all the classification problems. It's really something that is a very common data set that had turned out to have a good license that we could ship it with Scikit learn and basically we build most of the examples using this Iris data set which is also very much using text books of machine learning. So, that was kind of the default choice and it talks to people, because it understands what's the problem that you are trying to do and it's rich enough and not too big so we can- make all these examples super fast and build a nice-
That's cool, what is the data set? Like, what exactly is it about?
So the Iris data set you are trying to predict the types of plants, for example using the sepal length and the sepal width, so you have a number of features that describe the plant and you are trying to predict which one among 3, so it's a 3 label, 3 class classification problem.
Yeah, that's cool. Enough data to not just be a linear model or something, a single variable model but not too much?
Exactly, it's not completely linear, a bit, but not too hard at the same time.
Right, if you get 20 variables that's probably too much to deal. Then one is on diabetes. What about diabetes, what does that data set represent?
I am actually not really sure what's the- now it's a regression problem, it's used a lot in the linear model especially for the spots regression models because the, I mean, part of these spots regression models that try to extract the predictive features, I guess in the diabetes data set you try to find something related to diabetes and you are interested in finding the most predictive features what are the best features, and that's part of the reason I think we are using it.
And then another one is digits, which kind of is to model images, right?
One of the early I would say breakthroughs of the machine learning was this work int he 1990s where 39:01 were trying to build a system that could predict what was the digit present on the screen on in the image. So, it's a very old machine learning problem where you start from a picture or image of a digit that is handwritten and you try to predict what is it from 0 to 9. And it's an example that basically people can easily grasp in order to understand what is a machine learning. You give me an image and I'll predict something between 0 and 9. And historically, when we did the first version of the Scikit- learn website, we had something like seven or eight lines of Python code that were running classification of digits. So, that was kind of the motivation example where we said ok, Scikit learn has machine learning made easy and here it is an example, ten lines of code classifying digits and that was basically the punchline.
Solving this old, hard problem in a nice simple way, right?
Yeah.
You know, lately there's been a lot of talk about artificial intelligence, and especially from people like Elon Musk and Stephen Hawking, saying that maybe we should be concerned about artificial intelligence and things like that. So one of my first questions is around this area is, is machine learning the same thing as artificial intelligence?
Depends who you ask.
Sure.
I mean, yeah, it was basically the early mane of trying to tech us a computer to do something. It dates back from the 1960s and 1970s where basically the US example, MIT had labs that were basically called AI labs. And machine learning is kind of the I would say more restricted set of problems, that compared to AI which is say when you do AI and you want to do work with text or linguistic you want to build a system that understands linguistic. That would be an AI problem. But machine learning is kind of a saying ok, I've got a loss function, I want to optimize my criteria, I've got something that I want to train my system on and in the sense you teach a system to learn and so you create some kind of intelligence but it's not I would say simpler thing to say than saying intelligence which is kind of a hard concept. That's maybe my personal answer to this.
Yeah, and it's a great answer. Just from my limited exposure to it, it seems like machine learning is more about classification and prediction, whereas the AI concept is- there is a strong autonomous component that is just completely lacking for machine learning.
Yeah, I guess I would explain it simply like this exactly.
What things have you seen people using Scikit- learn for that surprised you? Or oyu were like wow, you guys are doing that, that's amazing.
So, in Scikit- learn we have this testimonial page where we ask typically companies or institute that are using Scikit- learn to like write a couple of sentences to say what they are using Scikit- learn for and why they think it's great, and trying to find this, and I remember there was this dating website saying that they are using Scikit- learn to optimize dates between people, so that was like a funny one.
And it is funny. So there may be people out there who are married and maybe even babies were born because of the Scikit- learn.
Yeah, that would be great, I'm going to add this to my resume.
Matchmaker. So, if people want to get started with Scikit- learn, they are listening and they are like, wow this is awesome, where do I start, like what would you recommend for sort of getting into this whole world of machine learning and getting started with Scikit- learn in particular?
The first start is the Scikit- learn website which is pretty extensive. But also a lot of tutorials that I have been giving by core devs of Scikit- learn in different conferences like SciPy, Euro SciPy, you can find all these videos online. And just take some of them and just sit down and just listen and try to do it yourself afterwards. I mean, for example in SciPy you get tutorials on Scikit- learn that are pretty much a whole day of tutorials, and which is hands on, so you can really look and get the materials online from the tutorial and get started.
Oh, that's excellent. Yeah, I think it's really amazing these days that there are so many of these videos online that you can- there is some topic you imagine like hey I want to know this thing in Python, there is very good chance that someone gave some kind of conference talk on it and it's online.
Yeah.
Anything you want to give sort of shout out to or final call to action before we sort of wrap things up a  bit?
So, if you have free time, you like machine learning, come give us a hand to maintain this cool library.
Yeah. Absolutely. Yeah, like I said, there is 457 contributors, but you know, you guys are looking to stabilize things and move forward so I'm sure there is a lot to be done around that.
I mean, there are two types of contributors. Like you have this one time contributors that are really expert in something that contribute something that is really specific and valuable that gets merged to the main code base, and you have I would say less people that are investing their time to read the code from others, keep the library consistent in terms of API and that's really this big reviewing work that I would say the historical core devs of Scikit- learn are pretty much mostly doing this these days, and invest little time to do really new stuff that is basically left to the newcomers and I think what would be if I had to wish something for the future is that these people were these onetime contributors also spend a bit of their time to help us maintain the entire library, in longer run.
Yeah, that makes sense. I can see in something like Scikit- learn where it's kind of a family of all these different algorithms and little techniques that if you want to add your technique you just go in there and you do that a little bit and you kind of stay out of the rest of the code, and that I can see how that would definitely lead to inconsistencies and so on.
Yeah. And the policy, I mean, in terms of- Scikit- learn maybe there is less things that are coming in these days is that we are not trying to build a library that contains all the algorithms that you can ever think of what I get published every year; we are trying to keep or have the algorithm that are better on some clear use case in the current state. So, we cannot implement everything but at least if you have a particular type of problem, you should have something in Scikit- learn that does a good job.
So, before I let you go, I have two more final questions for you. So if you are going to write some Python code, what editor do you open up?
So I've been a big user of text mate over the years, and after that I switched to Sublime recently because the I got convinced by my neighbors. So no Vim or Emacs.
Yeah, that's cool. I like Sublime text a lot, very nice. And, of all of the cool machine learning and Python in general packages out on PyPi, what are some that you think people maybe don't know about that you are like hey this is awesome, you should know about it.
Maybe I'm buzzed , because I mean do a lot of machinery for brain science, and I've been working for the past 5 years on this project that was called MNE, which allows you to process brain waves and classifying brain stage like for example build a brain computer interfaces or analyze clinical data of the electrophysiology. If you want to play with the brain waves you can check it out.
That's really cool. And when you say brain machine interface, is that like EEGs and stuff like that?
Exactly. EEG, MEG.
Ok. Very awesome, I haven't heard of that one, that's cool.
That's more my second baby.
That's great. So, Alexandre, it's been really great to have you on the show, and this has been super interesting conversation, thanks.
Thank you very much.
You bet. Talk to you later.
