We all know that LLMs and generative AI have been working their way into many products.
Well, it's Jupyter's turn to get a really awesome integration.
We have David Kui here to tell us about Jupyter AI.
Jupyter AI provides a user-friendly and powerful way to apply generative AI to your notebooks.
It lets you choose from many different LLM providers and models to get just the help that you're looking for.
And it does way more than just add a chat pane in the UI.
Listen in to find out.
This is Talk Python to Me, episode 440, recorded October 30th, 2023.
This is your host, Michael Kennedy.
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython, both on fosstodon.org.
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.
We've started streaming most of our episodes live on YouTube.
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of that episode.
This episode is sponsored by Posit Connect from the makers of Shiny.
Just share and deploy all of your data projects that you're creating using Python.
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards, and APIs.
Posit Connect supports all of them.
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.
And it's also brought to you by us over at Talk Python Training.
Did you know that we have over 250 hours of Python courses?
Yeah, that's right.
Check them out at talkpython.fm/courses.
David, welcome to Talk Python to Me.
Awesome to have you.
Yeah, thank you, Michael.
I'm really excited to see what the AIs have to say today.
The AIs, yeah, language models, sure.
Yes, exactly.
Exactly.
Now, you've built a really cool extension for Jupyter that plugs in large language models for people and it's looking super interesting.
So I'm excited to talk to you about it.
Yeah, I'm excited to talk about Jupyter AI too.
I've actually presented this twice at one set, actually three times.
I did a short demo at like this like tech meetup thing in Seattle.
That was actually the first time Jupyter AI was shown to the public.
And then I presented at PyData Seattle at Microsoft's Redmond campus.
And then I got to present again at JupyterCon in Paris this May.
It was a really wonderful experience.
But yeah.
Wow.
Yeah, you're making the rounds.
Yeah.
I love to talk about Jupyter AI.
It happens to get me some plane tickets.
Just joking.
Honestly that's like half the bonus of conferences is like awesome places you get to go.
The other half probably is the people you meet.
You know, it's really cool.
Oh, for me, it's like almost like all the people.
Like the people are just so great, especially JupyterCon.
Yeah.
And the JupyterCon videos are now out for JupyterCon 2023.
And there's a ton of good looking talks there.
So yeah, lots of really smart people.
I mean, like I was chatting to a few folks there and that's like the only place where you're going to find like people who work at these hedge funds and trading firms just lounging so idly and casually.
Right.
Like, yeah.
The market's opening and they're chilling.
It's all fine.
Yeah.
There's a lot of smart people there.
Yeah.
Jupyter more than a lot of programming technologies bring people from all sorts of different places together, different backgrounds.
There's like a huge.
So yeah, there's like a lot of reasons behind that.
But long story short, Jupyter is pretty awesome.
And that's kind of why I work to contribute to it.
Awesome.
Well, let's start this whole conversation with a bit of background about yourself.
And for people who didn't see your talk and don't know you yet, tell them a bit about you.
I didn't really give much of an intro there either, but sure.
Yeah.
So I've worked for AWS as a software engineer and I've been with AWS, specifically the AI ML organization at AWS.
I've been with them for almost two years now.
Right now, my manager is actually Brian Granger, who's the co-founder of Project Jupyter.
He also works for AWS.
Yeah.
So he's been offering some technical and product guidance for the things that we're building.
And he's a fantastic gentleman to work with.
Oh, that's a really neat, need to have him available as a resource, you know, as a colleague.
Yeah.
You know, it's funny.
Like I actually, yeah, I met him internally.
So when I first joined, I wasn't working for him, but at tech companies, you can do this internal transfer thing.
And basically my old team was kind of, the team I just joined right after I joined, it sort of started to dissolve a little because they just launched a product at reInvent, which happens in like December.
And then, so I joined in December and it's like, oh, hi.
So then I, yeah.
And then I joined, I messaged, I saw just, saw Brian Granger's name somehow.
I messaged him and I didn't even know that he was the co-founder of Project Jupiter.
I just wanted to work for him because I used it before.
And yeah, it's a pretty funny story.
Indeed.
I imagine that project has, has really, you know, this Jupiter AI is a great example, but I just, thinking of being say a founder of Jupiter or, or something like that, these things take on a life of their own.
And he's probably in awe of all the stuff happening and all the things going on.
And there's probably a lot of stuff in Jupiter.
He doesn't even know about, right.
It's like that's happening.
Yeah.
It's huge.
And yeah.
And like the leadership structure has changed to accommodate that.
So Brian is no longer the benevolent dictator for life.
Jupyter Project Jupyter is now governed by a committee, decentralized and democratized just to allow it to scale.
Yeah, of course.
Let's start by talking about a bit of the role of AI in data science.
I don't know how you feel about it.
You must be somewhat of an advocate putting this much time and energy into bringing it to Jupyter.
Wow.
However, personally, when I, I want to know something, I don't think there's a great specific search result for it straight to ChatGPT or friends.
I think there's, there's such a, such a wealth of information there from, I need to take this paragraph and clean it up and make it sound better to I have this program.
I want to convert to another language or I have this data in this website.
How do I get it?
You know, like just, you can ask so many open-ended questions and really get great answers.
So it seems to me, especially coming from, like I mentioned before, those diverse backgrounds, people not necessarily being like super deep in programming, maybe they're deep in finance, but they do programming that having this AI capability to ask like, Hey, I know I can, but how, what do you think for data science in particular?
This is an interesting topic, right?
Because I think the whole power of language models stems from their ubiquity and versatility and how they can sort of be very generally applicable.
So like the thing about language models is that they're basically statistical models that have been trained on a very, very, very large corpus of data.
And that's really to the computer.
The computer doesn't really understand English.
It doesn't really understand natural language.
And it has, it's basically making, it has like, when it's trained, it has like knowledge of the distribution of information and how information sort of interacts with other information.
Because of that, it's a very, it has a very general applicability, right?
And I don't think that's utilities limited to data science.
Now if we're talking about the field of data science specifically, I think language models have extraordinary utility and explanatory natural language tasks, which I think everybody is aware of now, now that a ChatGPT has been out for almost a year.
But I think in the field of data science and other like deep technical fields, they're especially applicable because of how complicated some of the work is.
Chat AI can also help analyze and debug code, which JupyterLab also allows you to do.
I know it's statistics, but when you look at it, it seems like it understands, right?
It seems like it understands my question.
And I think one of the really interesting parts is the fact that these LLMs have a context.
I would like you to write a program in Python.
Okay, great.
Tell it to me.
I want a program that does X, Y, Z, and then it writes it in Python, which sounds so simple.
But up until then, things like Siri and all the other voice assistants, they seemed so disjointed and so not understanding.
I just asked you about the weather.
And when I ask you how hot it is, how do you not understand that that applies to the weather?
The fact that you converse with them over a series of interactions is pretty special.
The context, yeah, it's basically implemented just by passing the history, the whole history appended to your prompt.
So yeah, it's not like any super staple magic or whatever, but it's still very interesting like how far you can take it.
And yeah, definitely like the context allows you to interact with the AI a lot more conversationally and humanly.
Like you don't have to pretend like you're talking to an AI.
You can actually just kind of treat it like a human and it still answers questions very well.
Yeah, it was even at Google, there was that engineer who said they thought it had become sentient and there was that whole drama around that, right?
This is such a crazy coincidence, but my roommate is actually friends with that gentleman.
Oh really?
I know.
Wow.
Absolutely crazy coincidence.
Like, I just thought it was really funny you bringing it up.
It was a Cajun gentleman, a senior engineer at Google, right?
Yeah.
Very funny, mate.
It's been a little while.
I don't remember all the details, but yeah, I mean, it's pretty wild and pretty powerful.
I think I've recently read, I'm trying to quick look it up, but I didn't find it.
I think they just used LLMs to discover like a new protein folding.
And it's that kind of stuff that makes me think like, okay, how interesting that knowledge wasn't out there in the world necessarily.
I have a lot to say on that subject.
So personally, I don't believe that language models are actually intelligent.
I think that people are conflating, well, they're certainly not conscious, right?
Absolutely.
Yeah.
As to whether they're intelligent, I don't think they are.
I think that intelligence has a, like intelligence as we know it, as humans know it, has some different characteristics that language models don't really exhibit.
They're best thought of as like really, really, really good statistical models.
Like are you familiar with the mirror test?
Maybe but I don't think so.
Yeah.
So it's like this idea on animal psychology, but like if a cat sees a mirror, it thinks it's another cat because it doesn't recognize its own inflection.
Right.
So it's like, oh, they all get like big and like, we're trying to like act big and tough to chase it off.
And it's just them.
Yeah.
Language models are kind of like that for, but for humans, right?
Like if something mimics human like quality closely enough, very tempting to think of it as human.
Yeah.
We see faces on Mars when it's really just erosion, stuff like that.
Exactly.
I could talk about this like for a full hour, but yeah, we should totally move on to another topic before I go on a tirade.
Well, when you're saying it's not that intelligent, I'm wondering if maybe you've mis-misnamed you this project.
Maybe it should just be Jupyter A, like the eye.
Do you got to drop the eye?
I don't know.
We're just following convention, right?
So I still use the term AI.
Of course, you got to talk to people.
Yeah.
Emphasize the artificial, huh?
Before we get to Jupyter AI, what came before?
How did people work with things like ChatGPT and other LLMs in Jupiter before stuff like Jupiter AI came along?
I think initially it was a combination.
So the initial motivation for this project came through a combination of sort of a demo put together by Fernando Perez, who is another, I believe.
I think he's another co-founder.
Another co-founder of Project Jupiter.
And he put together this demo called Jupite, which is, it's spelled like Jupiter, except the last letter is an E. And it's a pun of like ChatGPT, right?
Ju-pi-ti.
So it was a combination of that demo project set by Fernando and some motivation from my manager, Brian, who also was, you know, as a leader in the AWS AI organization, you know, he's always trying to think of fancy schmancy new ideas, right?
And this is a pretty fun idea to work out.
So I put together, I think this was sometime in early January, I put together the first demo, it was private, and I showed it off to the team and they were like, wow, this has a lot of potential.
Let's see if we can grow it a bit more.
And then as we worked on it for the next few months, it became clear like, oh, wow, this is actually, it's actually really significant.
Let's keep working on this.
So it's definitely been a collaborative effort to bring a Jupyter AI to where it is today.
Sounds like it.
Definitely a lot of contributors over on the GitHub listing.
Let's get into it.
What is Jupyter AI?
I mean, people can guess, but it's also different in ways than maybe just plug it in a chat window.
Jupyter AI is actually, right now it's two packages, but it's best thought of as just a set of packages that bring generative AI to Project Jupyter as a whole.
So not just JupyterLab, but also Jupyter Notebook and IPython.
Even the shell, how do you, I guess you invoke it by doing like the magic there as well.
It's the IPython shell, which is not the same as like a bash shell for instance, in a terminal.
You can dive into a little bit more detail on what these two packages are.
So we have the base Jupyter AI package, which is spelled exactly as you might imagine it.
It's Jupyter-AI.
That is a JupyterLab extension that brings a UI to JupyterLab, which is the screenshot that you're showing on your screen, but for viewers without a screen, it basically is the package that gives, adds that chat panel to the left-hand side and allows you to speak conversationally with an AI.
And then the second package is Jupyter-AI-Magix, which is spelled the same, except at the end, it's spelled hyphen-magix.
And that is actually the base library that implements some of the AI providers we use and brings things called magic commands to the IPython shell.
And magic commands basically let you invoke the library code, aka like calling, using it to like call language models, for instance.
And that allows you to do it inside an IPython context.
So what's crazy is that if you run IPython in your terminal shell, you can actually run Jupyter-AI from your terminal, which is pretty cool.
Yeah, I didn't realize that.
I mean, it makes sense, of course, but I hadn't really thought about it.
Yeah, I thought more of this like kind of a GUI type of thing that was alongside what you were doing.
Yeah.
We try to make it flexible and there's reasons for the magic commands, which I can, I can talk about that later though.
Sure.
This portion of Talk Python to Me is brought to you by Posit, the makers of Shiny, formerly RStudio, and especially Shiny for Python.
Let me ask you a question.
Are you building awesome things?
Of course you are.
You're a developer or data scientist.
That's what we do.
And you should check out Posit Connect.
Posit Connect is a way for you to publish, share, and deploy all the data products that you're building using Python.
People ask me the same question all the time.
Michael, I have some cool data science project or notebook that I built.
How do I share it with my users, stakeholders, teammates?
Do I need to learn FastAPI or Flask or maybe Vue or React.js?
Hold on now.
Those are cool technologies and I'm sure you'd benefit from them, but maybe stay focused on the data project?
Let Posit Connect handle that side of things?
With Posit Connect, you can rapidly and securely deploy the things you build in Python.
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quadro, Reports, Dashboards, and APIs.
Posit Connect supports all of them.
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise requirements.
Make deployment the easiest step in your workflow with Posit Connect.
For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm/posit.
That's talkpython.fm/POSIT.
The link is in your podcast player show notes.
Thank you to the team at Posit for supporting Talk Python.
Now one thing, kinda, you said this, but I want to emphasize it a little bit in that this will run anywhere IPython kernel runs, which is JupyterLab notebook, but also Google Colab, VS Code, other places as well.
So pretty much it comes to you wherever your Jupyter type of stuff is.
Yeah.
And the same goes for your lab extension.
So the great thing about lab extensions is that they work anywhere where the product is just sort of built on top of JupyterLab, right?
So Google Colab is essentially what Google has is a, well, I can't, I obviously can't attest to what they're actually doing, but most likely it's like a set of extensions or CSS themes that are built on top of JupyterLab, but like the underlying code is still, is still JupyterLab.
It's still mostly JupyterLab.
So you can actually just install extensions and they work just fine, which is another reason why JupyterLab is just a pretty awesome.
Yeah, it sure is.
Yeah.
JupyterLab itself is basically a preselected, pre-configured set of extensions, right?
That's pretty cool.
That is true.
Yeah.
Giving preference to, or my showing just what I play with mostly, which is ChatGPT.
There's actually a lot of language models that you can work with, right?
One of the big things, and this is something I'll circle back to later, is that JupyterAI is meant to be model agnostic, meaning that we don't discriminate against the choice of model or model provider, because as an open source project, it's very imperative that we maintain the trust of our users, right?
Like users have to be sure that this isn't just some product that exists to force them and force or pigeonhole them into using a certain model provider like OpenAI or Anthropic.
This, the product makes no opinions.
We simply try to support everything as best as we can.
And we've written a lot of code to make sure that all of these models and just play nicely together essentially.
Like every model provider, like let's say from Anthropic or AI21 or Cohere, every one of these APIs kind of has its own quirks.
Every one of its Python SDKs has its own quirks.
And we work very hard to basically iron out the surface and make everything have the same interface.
We can talk about that later though.
- Sure, and it also makes it pretty easy to try them out, right?
If you switch from one to the other, you're like, I wonder how Hugging Face versus OpenAI would do to solve this problem, right?
- Absolutely.
Like that's kind of one of the ideas is like, while certain model providers, they might offer a UI if they're very well funded by their investors, for example, OpenAI to have a UI, but that UI only allows you to compare between different models from OpenAI, which as an independent third party looking to use an AI service, that information is obviously a little biased, right?
You want to see what other providers have to offer.
What does AI21 have?
What does Anthropic have?
And right now, there really is no cross model provider UI or interface in general.
But that's kind of one of the use cases that Jupyter AI was intended to fit.
- Yeah, provides a standard way to interact with all these and sort of compare them.
And it's also a UI for them if you're using JupyterLab, yeah.
I'm not familiar with all of these different models and companies.
Do any of those run locally, like things like GPT4ALL, where it's a local model versus some kind of cloud?
Where's your key?
Where's your billing details and all that?
- We actually recently just merged a PR that adds a GPT4ALL support.
That's included in the release.
However, back when we first implemented this a few months ago, I had a few issues with the platform compatibility.
So like some of the binaries that we downloaded from GPT4ALL didn't seem to work well on my M1 Mac, for instance.
I'd say, yes, we do have local model support, but it's a bit experimental right now.
We're still like ironing out the edges and testing, like seeing how we can make the experience better.
Like does it sometimes only get bad output because we forgot to install the shared library?
Those are the type of questions that our team is wrangling with right now.
- I see.
So maybe, I just jumped right into it, but maybe tell people what GPT4ALL is just real quickly.
- GPT4ALL offers a few local models.
Actually they offer several, I believe, not just a few.
- I think they offer maybe like 10 to 15.
It's the numbers getting quite large.
- Yeah.
- To the point where I don't know what the right choice is.
I'm like, which one do I download?
They say they're all good.
Of course they're going to say they're good.
- I'll be frank.
I don't actually have that much experience with GPT4ALL.
We mainly use them as sort of a provider for like these free and open source language models.
I think they offer a UI as well for multiple platforms.
- I've only played with them a little bit, just started checking it out, but it's basically a local, you download the model, you run it locally, you don't pay anything because it's just running on your machine, right?
As opposed to say OpenAI and others where you've at least got rate limiting and certain amount of queries before you have to pay and maybe potentially access to better models like ChatGPT4 versus 3.5 and so on.
- That's also taking the characteristics of these service providers for granted, right?
So yes, definitely, while it does hurt the wallet to pay for usage credits, right?
It's also pretty remarkable how small the latency has gotten with some of these APIs.
I've gotten like sub 500 millisecond latency on some of these APIs.
That's really incredible because when I was using GPT4ALL, the latency was a little bit high, right?
When running locally with limited computer resources.
It's really remarkable like how fast these APIs are.
- It is pretty insane.
Sometimes it drives me crazy.
I'm just only again, referring to ChatGPT because I don't have the experience with the others to the degree, but it drives me crazy how it artificially limits the response based on the speed of the response.
So it looks like it's chatting with you.
I'm like, no, I have four pages of stuff because you just get it out.
It'll say something like, if you can ask, I gave you a five page program.
Let's call it X.
If you say, what is X?
It'll just start printing it slowly line by line.
You know, you just are echoing it back.
Just get it out.
I want to ask you the next question.
You know what I mean?
- In that case, that's actually a feature request that we've gotten because it doesn't actually slow down.
Like, it's not just like a pointless animation.
Yeah.
The servers are streaming essentially token by token, right?
As the language model generates output.
So it's kind of more like a progress indicator than a superfluous animation.
Yeah.
- Yeah, of course.
But if you've got something large, large blocks of text you're working with, it can be, it can be a drag.
All right.
I wanted to kind of touch on some of the different features that I pulled out that I thought were cool.
I mean, obviously it goes without saying that Jupyter AI is on GitHub.
I mean, because it's software.
So it's open source, which I don't know if we said that, but obviously free open source on GitHub, BSD3 license.
But it's also noteworthy that it's officially under the JupyterLab organization, not under the, the David, or David account.
You know what I mean?
- It's officially part of the JupyterLab sub project.
And yep, as you pointed out, we're under the JupyterLab GitHub org as well.
- Yeah, that's awesome.
Let's talk about some of the different things you can do with it.
Some of them will be straightforward, like just like, how do I write a function, Jupyter AI?
And others I think are going to be a little more interesting.
So let's start with asking something about your notebook.
Tell us what people can, can do here.
- Asking about your notebook basically means like you can actually teach Jupyter AI about certain files, right?
So the way you do this is via a Slack command in the chat UI that you type slash learn and then file path.
And you essentially teaches the Jupyter AI about that file.
Now it works best with files are written in natural language, right?
So like text files or markup or markdown rather.
Yeah.
So like those, like, especially like developer documentation as well, right?
It works really well with those, it works best with those kinds of files.
And after Jupyter AI learns about these files, you can then ask questions about the files.
It's learned by prefixing your question with the slash ask.
- That is so cool.
- It is pretty cool, I know.
- It's so cool because what I've done a lot of times, if I want ChatGPT to help me, it's like, I'm like, all right, well, let me copy some code.
- Right.
- Then I'm going to have a conversation about it.
But a lot of the context of, well, it's actually referencing this other function and what does that do or just a broader understanding of what am I actually working on is missing, right?
Because I've only copied it.
You can't paste, you know, 20 files into ChatGPT and start talking about it.
But with this, you can, right?
You can say, learn about, you can say, learn about different things, right?
You can say, learn about your notebook, but you can also probably tell it like, learn about my documentation or learn about my dataset.
And now let me talk to you about it.
What's interesting is that the right now, while it works best for natural language documents, we are working on improving the experience for code.
From our testing, like the code is mostly the capabilities of Jupyter AI after learning code is right now mostly limited to explaining what code does, but sort of like explains it from the doc stream.
So we're working on a way to format the code in a manner that is more interpretable to a language model.
We're working on ways to improve the experience for code, but yeah, definitely the long-term vision is to have Jupyter AI literally be able to learn from a whole directory of files or possibly even a URL, like a remote URL to like a documentation page.
We have some big ideas there.
We're still working on them.
I want to work with a new package XYZ.
Like I don't know what XYZ is.
You know what?
Here's where you can learn about it.
Go and figure it out.
Like query the PyPI API, get the docs page from the metadata, and then go to that URL, scrape it.
Like lots of things we're exploring.
It's still kind of early days for this, right?
You've been at it about a year or so?
It's been out for a while.
Recently I've had to work on a few other things as well, like Jupyter AI.
Unfortunately I cannot give my entire life to Jupyter AI.
So I've been working on a few other things these past few months, but yeah, there are a lot of things that I envision for Jupyter AI.
I have a much bigger vision for what I want this project to be in, what it can be capable of.
Exciting.
So what did this screenshot that you got here in the section that I'll link to in the show notes is cool because you can select a piece of a portion, not even a whole cell, but a portion of code in a cell.
And then you can ask, what does this code do?
We have an integration with the JupyterLab editor APIs.
So you can select a block of code and then include that in your prompt.
And it will be appended to your prompt below, right?
It's appended to prompt, sorry.
You can basically ask, so you can select a block of code.
So in this screenshot right here, there's this block of code that computes the least common multiple of two integers, right?
And you can select that and then click include selection and then ask Jupyter AI, what does this do?
Which is pretty awesome.
Another checkbox is replace selection.
I'm guessing that is like, help me rewrite this code to be more efficient.
Or if there's any bugs, fix it.
So the replace selection checkbox is totally independent.
So both, so you can actually use both at the same time.
And one of the use cases for this is refactoring.
And I've actually applied this in practice a few times, where you can basically select a block of code, and then click both include and replace selection.
And then you can pull out your prompt to say, refactor this block of code, do not include any additional help or text.
And when you send that prompt over, it will actually refactor the code for you in your notebook, which is, yeah, like is, is pretty great.
That's pretty awesome.
You know, you could do things like refactor this to use guarding clauses.
So it's less nested, less, less arrow code or whatever, right?
Yeah.
Or like add a docstring, right?
Summarize this purpose of this function, and then enclose that in a docstring and add it to the function.
Right.
Or this code is pandas code, but I'd like to use pollers.
Please rewrite it for pollers, which is not a super compatible API.
It's not like DAST to pandas, where it's basically the same.
Yeah.
And this kind of circles back to that question that you had asked earlier.
I think I went on a tangent there and didn't fully answer, but like, what is like the utility of Jupyter AI to like data practitioners, right?
So we're talking data scientists, machine learning engineers, like this, the include selection features, we've heard great feedback about how helpful it is to like actually explain a data set.
So sometimes like you're working with a test set and it's not immediately clear what the features of this test set are, or like what this even does, because sometimes it's like high dimensional data and they can literally select it and then click include selection and say, and tell Jupyter AI, explain to me what this does.
Just like, what, what is this like data frame stuff?
Like, whoa, we got data frames in data frames.
Like what's going on here?
Like what even is structure?
That's awesome.
And I think it's super valuable.
And this is like a little bit I was getting to before one of the features that I think is cool.
Whereas if you just go with straight ChatGPT, you copy your code, you paste it into the chat.
Hopefully it doesn't say it's too much text and then you can talk about it.
But then when you get an answer, you've got to grab it, move it back over.
And this just, this fluid back and forth is really nice.
Yeah.
And that's actually one of the design principles that we worked out when first starting this project officially.
Was the idea that Jupyter AI should be human centered as in you shouldn't be expected to be a developer to know how to use this tool.
Like this tool is for humans, not, not for any specific persona, just for humans in general.
That's awesome.
Yeah.
So in this case, you select the function that does the lowest common denominator bit and you ask it what it does.
It says the code will print out the least common multiple of two numbers passed to it.
Super simple, very concise.
Okay, great.
Now we can go on to the next thing, right?
Yeah.
There's a, this LCD function that, that we're kind of talking about here.
This example is recursive, which I think recursion is, is pretty insane, right?
As a, just a concept for people to get their head around.
This is the iterative version.
So this is after they, yeah, this is the iterative.
Oh, this is after.
Yeah.
So if we go back up, Oh, one of the things you ask it is things like, and the example is rewrite this function to be iterative, not recursive.
Right.
That's really, really awesome.
Right.
You're like, this is breaking my brain.
Let's, let's see if we can not do that anymore.
This portion of Talk Python to me is brought to you by us.
Have you heard that Python is not good for concurrent programming problems?
Whoever told you that is living in the past because it's prime time for Python's asynchronous features with the widespread adoption of async methods and the async and await keywords.
Python's ecosystem has a ton of new and exciting frameworks based on async and await.
That's why we created a course for anyone who wants to learn all of Python's async capabilities, async techniques and examples in Python.
Just visit talkpython.fm/async and watch the intro video to see if this course is for you.
It's only $49 and you own it forever.
No subscriptions.
And there are discounts for teams as well.
Another thing I wanted to talk about, and you talked a fair amount about this in your presentations that you did.
I can't remember if it was the JupyterCon or the PyData one that I saw, but one of those two, you talked about generating new notebooks and how it's, it's actually quite a tricky process.
You've got to break it down into little steps.
Cause if you ask too much from the AI, it kind of doesn't give you a lot of great answers.
Tell us about making new notebooks.
Like, why would I even use, like I can go to JupyterLab and say, file new, it'll make that for me.
What's this about?
The generate capability is great because it generates a file that is essentially a tutorial that can be used as a tutorial to teach you about new subjects.
Right?
So like you could, for example, submit a prompt, like slash generate a notebook about async IO or a demonstration of how to use matplotlib.
And after, so this will take a bit of time, but eventually Jupyter AI is done thinking and generates a file and it names a file and generates a notebook and the notebook has a name, it has a title, it has like sections, table of contents, and each of the cells within it like is tied to some like topic that is determined to be helpful and to answer the user's question.
Awesome.
Could I do something like I have weather data in this format from the US weather service.
Could you generate me a notebook to plot this XYZ and help me answer these questions?
Or like, could I ask it something like that?
Not at the moment.
So like that would best be done.
That would be done best if since the data is already like, I'm assuming that the data is already available, right.
And some kind of like format.
So like in a notebook, you could use the chat UI to like select that entire, select that selection and then tell it, tell Jupyter AI to generate code to plot that data set.
So right now generate only takes a natural language prompt as its only argument.
So it's kind of like stateless in that regard.
So in this case, you can say slash generate a demonstration on of how to use Matplotlib.
And then the response is great.
I'll start working on your notebook.
It'll take a few minutes, but I'll reply when it's ready.
In the meantime, let's keep talking.
So what happens behind the scenes that takes a few minutes here?
This is a bit interesting.
It does kind of dive deep into like the technical details, which I'm not sure is, which do you want to just like dive?
Yeah, let's go tell us how it works.
Probably a good chance to explore that.
Yeah.
So slash generate.
The first thing is that the prompt is first expanded into essentially a table of contents.
So basically we tell the language model generate us a table of contents conforming to this JSON schema.
And when you pass a JSON schema included in your prompt, the language model will be much more, will have a much higher likelihood of returning exclusively an object, a JSON object that matches that JSON schema that you had initially provided.
So in our case, we generate a table of contents and then we take that table of contents.
And then we say for each section, we do this in parallel, like generate us some code cells that are appropriate for teaching this section of the document.
So for example, for Matplotlib, right?
Like maybe the first section is your first, like generating your first plot, plotting 3D functions.
And the next one is like plotting complex functions with face or something like that.
And then with each of these sections, we then send another prompt template to the language model for each of these sections, asking it to generate the code.
And then at the end, we join it all together and then we save it to disk and then emit that message and say, we're done.
Maybe the English literature equivalent would be, instead of just saying, write me a story about a person who goes on an adventure and gets lost.
It's like, I want, give me an outline, bullet points of interesting things that would make up a story of how somebody goes on an adventure and gets lost.
And then for each one of those, you're like, now tell this part of the story, now tell this part.
And somehow that makes it more focused and accurate, right?
The main limitation is that because we're model agnostic, language models are limited in how much output they can generate, right?
The issue we were running into when we were trying to do the whole thing all at once, like generate, meet a whole notebook, is that some language models just couldn't do it.
In an effort to, you know, sort of stay model agnostic, we deliberately implemented, we deliberately broke this process down into like smaller sub tasks, each with its own like prompt template in order to accommodate these models that may lack the same token size windows that other models have.
I think just even for ones that have a large token spaces, I think they still, the more specific you can be, the more likely you're going to get a focused result instead of a wandering vague result.
Teach me about math or teach me how to factor, you know, how to integrate this differential, solve this series of differential equations or physics.
Like you're going to get a really different answer to those two questions.
Yeah.
That's also, so it goes back to like a topic I didn't, I did want to call out, but I don't, I don't think we hit on it is that the chat UI actually does support rendering in both Markdown and LaTeX, which is a markup language for math.
So you can ask it both complex engineering and mathematical questions, like asking it to explain you like, yeah, so like there might be a demo here.
I'm not sure if it's on this page though.
So if I had a Fourier, fast Fourier transform in LaTeX and I put it in there and say, what is this, it'll say it's a fast Fourier transform or something like that.
Yes.
And it also works the other way around.
You can also use it to say like, Hey, explain to me what the 2D Laplace equation is, or explain to me like what, what does this do?
Right.
And it will actually generate and format the equation inside the chat UI, which is really remarkable.
I love that feature.
It's actually really awesome.
And it's also really appropriate for a scientific oriented thing like Jupiter, right?
The remarkable thing is that because chat UBT and like other such language models, like the ones from Anthropic and AI21, because like they are founded on the premise of where like their functionality comes from having such a large corpus of data, they know a remarkable amount of information.
So like we've tried like some example notebooks of quantum computing and explained those really well.
We try, I tried one of like the Black-Scholes options pricing model, we use them in financial engineering.
And it's really remarkable, like the utility that it offers just by being there in the side panel.
Like you essentially have like a math wizard available to you in JupyterLab all the time.
It's probably better than a lot of math professors in terms of not necessarily in the depth of one area, but you know, if you ask somebody who does like abstract algebra about real analysis, they're like, I don't really do that part.
Or if you ask somebody about real analysis, about number theory, like I don't really, you can hit on all the areas, at least a generalist professor sort of thing.
We talked about the slash learn command.
That's pretty excellent already and where that's going.
So I'm pretty excited about that.
Yeah, it actually does have a lot of interesting technical tidbits to it, like the implementation.
Okay.
Yeah, actually, this is one of the really challenging things with these chat bots and things.
For example, I've tried to ask ChatGPT, if I gave it one, just one of the transcripts from the show, I want to have a conversation about it.
It's too much.
I can't do it.
It's just one, one show and in doc, like in your documentation, there might be a lot of files in there, right?
More than just one transcript levels worth.
So that alone, I think is kind of interesting just how to ingest that much data into it.
Yeah.
You know, this is a very interesting subject and it actually is a bit complex.
I'm sure it is.
I think there are some other features you want to discuss.
Let's dive into this for just a minute.
Because I think it is interesting.
How do you make, because this makes it yours, right?
It's one thing to ask vague, like, tell me about the Laplace equation and how does it apply to heat transfer?
Like, okay, great.
I have a specific problem with a specific library and I want to solve it.
And you don't seem to understand about enough of it.
So it really limits the usefulness if it doesn't, if it's not a little closer to what you're actually doing.
And I think this brings it closer.
So yeah, tell us about it.
Language models aren't just governed by like their intelligence, however you measure that, right?
They're governed by how much context they can take.
So one of the reasons ChatGPT was so remarkable is that it had a great way of managing context through conversation history.
And that like seemingly small, like leap and like seemingly small feature, like is what made ChatGPT so remarkably disruptive to this industry is because of that additional context.
And we think about like extending that idea, like how do we give an AI more context, make it like even more human-like and personal?
Well, the idea is similar.
We add more context and that's what learning does, right?
And so the way learning works is that we're actually using another set of models called embedding models.
And embedding models are very, very underrated and the AI like modeling space, right?
These are really remarkable things.
And they have this one, I'll only cover like the most important characteristic of embedding models, which is embedding models take syntax and map it to a high dimensional vector space called a semantic space.
And inside of the semantic space, nearby vectors indicate semantic similarity.
I know that's like a lot of words, I'm going to break that idea down, right?
So like canine and dog, let's take these two words as an example, right?
These two words are completely different.
They don't even share a single character in similarity together, right?
They don't have a single letter in common with one another.
And yet we know as humans that these two dogs, these two words mean the same thing.
They refer to a dog.
So like they have different syntax, but the same semantics, the same semantic meaning.
So their vectors would be-
Would be mapped close.
Would be very close by whatever metric you're using, yeah.
If you extend this idea and like you imagine, okay, what if you split a document?
What if you split a file into like one to two sentence chunks?
And then for each of these like sentence, let's just say sentences, for example.
Let's say we split a document to sentences and then we take each of those sentences and then map, use an embedding model to compute their embedding and then store them inside of a vector store.
Like basically like a local database that has, that just stores all of these vectors in like a file or something, right?
Now imagine what happens if we then take a prompt, like a question that we might have, so that is an embedding.
And then we say to the vector store, okay, for this prompt embedding, find me all of the other embeddings that are close to this.
Well, what you've just done in this process is called a semantic search.
So it's kind of like syntax search, except instead of searching based off of keywords or tokens or other syntactic traits, you are searching based off the actual natural language meaning of the word.
This is much more applicable when it comes to like natural language prompts and natural language, like corpuses of data, because this is like the actual information that's being stored.
Now we don't care about the characters.
We care about the information that they represent, right?
The essence of it.
Yeah.
And these vectors are computed by the larger language model?
The vectors, the embeddings are computed by an embedding model and they're actually a separate category of model that we have our own special APIs for.
So in our settings panel, you can change the language model.
And I think we already discussed that, right?
Yeah.
But what's interesting is that underneath that, you'll also see a section that says embedding model and you can change the embedding model to like, we also offer that same principle of model agnosticism there.
Yeah.
This is very interesting.
Very interesting.
Let's talk a little bit about the format.
You said obviously that you can do LaTeX, which you say in, you say math, right?
You tell us, give me math.
Yeah.
Which is, yeah, it's pretty interesting, but you can do images, markdown code, HTML, JSON, text.
There's a lot of, a lot of different formats you can get the answer back in.
When you use the AI magics, we can pass them to like a renderer first before we show it to output to the user.
Yeah.
And with the AI magic, the percent percent AI in the cell, you can also specify, that's where you put the format potentially, but you can also specify the model and the service, I guess, for the provider.
The IPython magics are basically stateless in the sense that you always have to specify the model explicitly.
They don't operate off the premise that you are using JupyterLab.
They don't run off the premise that you have JupyterLab installed or are using the lab extension that we offer.
Because of that, like the model is stated explicitly every time.
That's by design.
When you sat down, what is your Jupyter AI provider set to?
What's your favorite?
As a developer, I like literally pick a random one to give the most test coverage at all times.
And that's actually a great way of finding bugs.
So yeah, I don't have a favorite one.
My favorite one is the one that works and hopefully that should be all of them.
You can also tell it to forget what I was talking about.
We're going to start over.
That's pretty interesting that you can do that along the way because you maybe had a bunch of conversations and we talked about the benefit of it, like knowing the history of that conversation, but you're like, all right, new idea.
Switching topics.
Chat.
I think the last one I wanted to talk about specifically was interpolating prompts.
Kind of almost like f-strings where you can put in the prompt text, you can put a variable and then other parts of your notebook or program can set that value.
Yeah.
Tell us about this.
You can define a variable in your IPython kernel, right?
So like, and that's just dumb, but just like how you define any other variable.
But what's interesting is that IPython is actually aware of the variables that you are defining.
So we can programmatically access that when we implement the magic, right?
Basically if you define any variable at a top level scope, like let's say a poet equals Walt Whitman, right?
So we have a name variable called poet.
And then you can send a prompt over like write a poem in the style of curly braces, poet and curly braces.
And when that is run, the variable, the value of that variable is interpolated and substitutes around the curly braces.
So the final prompt becomes write a poem in the style of Walt Whitman.
And when that prompt is sent, well, you can imagine it generates a poem of Walt Whitman.
Yeah.
The variable interpolation that we offer in IPython is very useful for like very quick like debugging.
So you can actually reference a cell in the notebook directly.
I think a lot of people don't know this, but like in a Jupyter notebook, there's like the in and out indicators to the left of each cell.
So it'll say like in.1, out.1, right?
So those are actual variables and you can use them here too.
So you can reference like debug, tell me why this code is failing, curly braces in.1.
That's you just on the screen there, just scroll there.
Imagine what this does, the in bracket 11 or what went wrong in out bracket 11 or yeah, something like this.
Right.
It's fine as long as you don't go and rerun that cell.
Yeah.
But like you said, this is not for long-term.
You can make it independent of the order of execution just by like assigning whatever variable that is to whatever like content that is to a named variable.
That way, no matter what order you run them in.
People might be thinking when you describe this interpolation thing, just bracket, bracket, curly brace, curly brace.
They're like, we already have that in Python.
You just put an F in front of the string and so on.
But this is in the message that goes to the AI cell magic, not straight Python, right?
That's the relevance.
That's why this is interesting, right?
Yes.
I guess it really comes down to the different models that you select.
So you opt into this a little bit, or maybe you need to understand it that way, but talk to us a bit about privacy.
If I select something and say, what does this do?
What happens?
Something important to emphasize here is that whenever you use a language model that's hosted by a third party, so like it's running on their servers, right?
Regardless of whether this model is free or not, like the fact that you're sending data to a third party over the internet, like that's where the privacy and security concerns happen, right?
So that happens whenever you're sending data across the wire over the internet.
But we have some special safeguards in place here specifically to assuage fears of concerns over privacy and security that a lot of open source users have.
And one of the important ideas here is that Jupyter AI is both transparent and traceable.
So when we send a prompt to a language model, that's always captured in the server logs by default.
So that's always being logged.
That's always being captured.
So it's always going to be traceable.
There's no secret back channel.
You tell people this is happening.
Okay.
Yeah.
So if an operator needs to audit, like, oh, dang, let me check just to make sure nothing scary was sent over to OpenAI.
Well, the operator can review the server logs and make sure that all usage is compliant with whatever privacy policy their company has.
And Jupyter AI is also exclusively user driven, meaning that we will never by default send data to a language model, even if you selected one, right?
Like, we will never send data to that language model until explicit action is done by the user.
So in this case, like clicking the send button, clicking shift, enter and running to sell.
Nothing is sent to language model or embedding model until that happens.
That's really all you can do.
That's sounds great.
Because you don't control what happens once it hits OpenAI or Anthropic or whatever.
That's why the transparency is so important.
Right.
And oh, I forgot to touch on traceability.
So like with these AI generated cells, right?
So like the output cells in the metadata, we indicate that with like the model that was used to generate an output cell, if it was if it comes from the Jupyter AI magic.
So that way it's also like traceable, not just in the logs, but like in the actual files, metadata and stuff as well.
That's cool.
So it'd be real easy to say, have the cell magic and then you know, notebooks store their last output.
Like if you upload them to GitHub, they'll, they'll keep their last output.
Unless you say clear all cell outputs, depending on which model you have selected, you might not get the same output, not even close to the same output.
So you might want to know like, well, how did you get that picture?
Oh, I had Anthropic selected and not the other, right?
Like that is really nice.
I've actually used the server logs myself to debug like prompt templates, for instance.
Right.
Because what we show in the logs is the full prompt, like after applying our template, after applying like the edits, like that's what's actually shown.
So that's really also really helpful for developers who need to debug what's happening.
Yeah, of course.
Or if you're a scientist and you're looking for reproducibility, I doubt there's much guaranteed reproducibility, even across the versions of the same model, but at least you are in the same ballpark, you know, at least I know what model it came from.
You can set the temperature to zero, but it won't generate a very fun output.
That is a workaround if you truly need a reproducibility.
I suppose.
Yeah.
The temperature being the ability to tell it how creative do you want to be or how focused do you want the answer to be, right?
It's a hyperparameter that basically governs the randomness, like how far away from the mean it's willing to deviate.
Kind of, yeah, vaguely describable as creativity.
Yeah.
Yeah, I suppose.
And if you're looking for privacy, the GPT for all might be a good option.
Oh yeah, absolutely.
For that, right?
Because that's not going anywhere.
Yeah.
However, some of them do have a license restrictions and that's also why we have also taken it this slow when it comes to adding more GPT for all support is because different models are licensed differently and that's another consideration we have to take in mind.
Yeah.
Yeah, of course.
You're playing in a crazy space with many different companies evolving licenses.
Yeah.
Let's close it out with one more thing here.
Maybe two more things.
One is there's a lot of interest in these agents and it sounds, for example, your create a notebook that does this sort of thing, like teach me about Matplotlib is a little bit agent driven thinking of like Lang chain and stuff like that, or even GPT engineer.
What's the story?
Do you have any integrations with that?
Any of those types of things?
We're working on one where basically you won't need to use slash commands anymore.
So this is like, again, like we're just kind of playing around with this, seeing how well it behaves.
But we are trying to use agents to, for example, remove the need to use slash commands.
So when you say like generate a notebook, like it will just generate one.
You don't have to know to slash command for that.
Like it will just go.
So like, yeah.
However, we don't have any, we don't use agents at the present moment now.
And the reason for that is that they have very, they're not very model agnostic, at least the ones from our research, like they only work well for a specific model and a specific prompt template, but beyond that, it's hard.
All right.
Last question running out of time here.
What's your favorite thing you've done with Jupiter AI?
Not like a feature, but what have you made it do to help you that you really like?
Teach Jupyter AI about Jupyter AI.
That's an easy one.
So we have the documentation.
What did it learn?
Well, it learned about itself.
So like I was at a conference and I, so this was at Py data and I actually got enough questions to the point where I did, I just downloaded, I had the documentation already available in my home directory because of some other previous or restructure tax them.
Yeah.
And the markdown source, the markdown source.
Right.
And then I just had slash learn.
I just learned that doc and then I just had that laptop on the side and told people, like if you have any questions, try it.
Just ask it.
Yeah.
In case, in case you don't want to wait in line.
So yeah, that was yeah, it's, it's pretty remarkable.
Yeah.
The learn feature by far is definitely my favorite and it's the one I want to spend the most time developing and pushing.
I think there's massive possibility there because I deeply understand what you're working on in a multifaceted way.
What are the documentations?
What is the code?
What is the data?
What is the network topology and servers I can work with?
Like all of that kind of stuff or pick your specialty.
Yup.
And that's how we make AI more human right before it takes over.
So no problem there.
Oh yeah.
Just kidding.
Let's wrap it up.
I think we're out of time, David.
So final question, some notable PyPI package, maybe something awesome you discovered to help you write Jupyter AI.
Anything you want to give a shout out to?
We definitely use LangChain a lot and that it's a pretty, it has some pretty fantastic integrations and we're actually built on top of LangChain really.
But also Dask.
Dask is really nice.
Yeah.
It has some great visualization capabilities for when you're doing parallel compute.
Like it has a great dashboard that's also available in JupyterLab.
Yeah.
That shows it running and distributing the work in JupyterLab is amazing.
And one of the contributors also reached out once he had heard I was integrating Dask into Jupyter AI.
He actually reached out to help us and offer like direct one-on-one guidance with using Dask.
And yeah, it's just been a fantastic experience using Dask.
Yeah.
I have no complaints.
It's, it's just pretty awesome that somebody has finally made parallel and distributed compute better in Python.
For sure.
Yeah.
Dask is cool.
Dask is very cool.
All right.
Well, final call to action.
People want to get started with Jupyter AI.
What do you tell them?
Install Jupyter AI or Conda.
Conda install works too.
It's on both.
Awesome.
Well, really good work.
This is super interesting and I think a lot of people are going to find value in it.
So I can see some nice comments in the audience that people are excited as well.
So thanks for being here.
Yeah.
Thank you so much.
Yeah.
See you later.
Bye bye.
Bye.
This has been another episode of Talk Python to Me.
Thank you to our sponsors.
Be sure to check out what they're offering.
It really helps support the show.
This episode is sponsored by Posit Connect from the makers of Shiny.
Publish, share and deploy all of your data projects that you're creating using Python.
Streamlet, Dash, Shiny, Bokeh, FastAPI, Flask, Quattro, Reports, Dashboards and APIs.
Posit Connect supports all of them.
Try Posit Connect for free by going to talkpython.fm/posit.
P-O-S-I-T.
Want to level up your Python?
We have one of the largest catalogs of Python video courses over at Talk Python.
Our content ranges from true beginners to deeply advanced topics like memory and async.
And best of all, there's not a subscription in sight.
Check it out for yourself at training.talkpython.fm.
Be sure to subscribe to the show.
Open your favorite podcast app and search for Python.
We should be right at the top.
You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.
We're live streaming most of our recordings these days.
If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.
This is your host, Michael Kennedy.
Thanks so much for listening.
I really appreciate it.
Now get out there and write some Python code.
[MUSIC PLAYING]
[MUSIC PLAYING]
(upbeat music)
[BLANK_AUDIO]
